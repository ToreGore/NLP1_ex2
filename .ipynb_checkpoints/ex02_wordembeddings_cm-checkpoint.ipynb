{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idMPWePBWZUB"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if pytorch can use the gpu\n",
    "\n",
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.device(0))\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing classes\n",
    "In this section, we build classes which are later used to load and clean data. We moved this to the top of the notebook to declutter the following section and to make it easier for you to follow our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-loading class\n",
    "It has three metods: <br>\n",
    "<ul>\n",
    "    <li><b>__init__(url):</b> Class constructor, takes an URL to GDrive;</li>\n",
    "    <li><b>load_csv():</b> Downloads a .csv from the GDrive link and returns it as pandas dataframe;</li>\n",
    "    <li><b>load_txt():</b> Downloads a .txt file from the GDrive link and returns it as a string.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to load data\n",
    "# following this stackoverflow post to download directly from Google Drive:\n",
    "# https://stackoverflow.com/a/56611995\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, gdrive_url):\n",
    "        \"\"\"\n",
    "        Class constructor\n",
    "        Args:\n",
    "            gdrive_url (str): URL to share Google Drive docs\n",
    "        \"\"\"\n",
    "        self.path = gdrive_url   \n",
    "    def load_csv(self):\n",
    "        \"\"\"\n",
    "        Returns DataFrame containing data from csv\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        url = requests.get(dwn_url).content\n",
    "        csv_raw = StringIO(url.decode('utf-8'))\n",
    "        df_ta = pd.read_csv(csv_raw)\n",
    "        return(df_ta)\n",
    "    \n",
    "    def load_txt(self):\n",
    "        \"\"\"\n",
    "        Returns String containing data from txt\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        #print(dwn_url)\n",
    "        url = requests.get(dwn_url).content\n",
    "        scifi = StringIO(url.decode('utf-8')).getvalue()\n",
    "        return(scifi)\n",
    "\n",
    "#txt = DataLoader(\"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\").load_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-cleaner class\n",
    "As suggested in McMahan (Natural Language Processing with PyTorch), we have proceeded by lower-casing each sentence, by removing the punctuation and the non alphabetic characters, also the stopwords have been stripped away from the text. <br>\n",
    "The strings get lowercased in order to avoid seeing the words at the beginning of the phrases as different words. <br>\n",
    "Whitespaces are used to tokenize the strings.<br>\n",
    "This class has three methods and no constructor except from the default one: <br>\n",
    "<ul>\n",
    "    <li><b>remove_nonalpha_chars:</b> Takes as input a pandas dataframe and the name of a column, proceeds to remove all non alpha-chars from said column. Returns the cleaned dataframe;</li>\n",
    "    <li><b>lower_casing():</b> Takes a pandas dataframe and the name of the column to operate on, all the uppercase chars become lowercase. Returns the lowercased dataframe;</li>\n",
    "    <li><b>remove_stopwords():</b> Removes all the stopwords from the column of a dataframe, both fields of the function. It returns the cleaned dataframe.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tore/venv/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal inspired by this stackoverlow answer\n",
    "# https://stackoverflow.com/a/43407993/7505264\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tqdm.pandas()\n",
    "\n",
    "class DataCleaner:\n",
    "    def remove_nonalpha_chars(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Removes non-alpha characters\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \", x))\n",
    "        return df_out\n",
    "    \n",
    "    def lower_casing(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Turns strings to lower case\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        \n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: x.lower())\n",
    "        return df_out\n",
    "    \n",
    "    def remove_stopwords(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Removes stopwords from text in English language\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: ' '.join([word for word in (x.split()) if word not in (stop) and len(word) != 1]))\n",
    "        return df_out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing class\n",
    "# df_ta['Review'].apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \",x))\n",
    "\n",
    "# rvws = {'Review': ['TEST_.']}\n",
    "\n",
    "# df_test = pd.DataFrame(rvws, columns = ['Review'])\n",
    "\n",
    "# df_test['Review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lth8ThtWHuc"
   },
   "source": [
    "# Loading data\n",
    "Here the two datasets gets downloaded directly from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tMgyvheSWKq2"
   },
   "outputs": [],
   "source": [
    "# url for tripadvisor_hotel_reviews.csv\n",
    "# following this stackoverflow post to download directly from Google Drive:\n",
    "# https://stackoverflow.com/a/56611995\n",
    "\n",
    "### URLS\n",
    "orig_url_ta = 'https://drive.google.com/file/d/1ihP1HZ8YHVGGIEp1RHxXdt3PPIi12xvL/view?usp=sharing'\n",
    "orig_url_scifi = \"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\"\n",
    "\n",
    "### DataLoaders\n",
    "TravAdvDataSetLoader = DataLoader(orig_url_ta)\n",
    "ScifiLoader = DataLoader(orig_url_scifi)\n",
    "\n",
    "### CSV and txts\n",
    "df_ta = TravAdvDataSetLoader.load_csv()\n",
    "scifi_txt = ScifiLoader.load_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DbD_oHDW0_7"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the datasets have been downloaded correctly\n",
    "This is done by printing in the first case the head of the pandas dataframe, in the second case by printing the first 500 chars of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "DFspOK98W0Nq",
    "outputId": "6f473269-6e50-4c34-8467-def67c1bc926"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20491"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sci-fi story gets turned into a dataframe to allow a more proper cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_dict = {\"Text\": [scifi_txt]}\n",
    "scifi_df = pd.DataFrame.from_dict(scifi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15388019"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scifi_df[\"Text\"].iloc[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJNUSouXZzBI"
   },
   "source": [
    "**Observations**:\n",
    "\n",
    "\n",
    "1.   Some reveiw include the rating (i.e. 4*). This should be removed\n",
    "2.   The last line has a typo (and probably many other lines too) which add noise. A correction of all errors, however, is not realistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "253JGx_BcJN2"
   },
   "source": [
    "Now, we look for all characters used in the reviews to get an idea of how we need to preprocess the data. We can see that there are no foreign language characters in the data but a couple of symbols, special characters and emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JMqXL4FctBZ"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CJVT_mwcwMg"
   },
   "source": [
    "For the data preprocessing, we first create a class that helps us to clean the data (following the OOP approach).\n",
    "\n",
    "**Note**: We only perform operations on the complete data set (training + test set) which do not lead to information leakage. Removing certain characters from the test set is a valid operation that also occurs in real world setting. Data is usually preprocessed before predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data in the step below, we compare the results of the second review to confirm that the cleaning was successful.\n",
    "We realize that most comments contain typos and that some typos like \"did n't\" result in single characters in the corpus. Given that we cannot correct every typo, we accept this noise in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nxy10s1tc9Yx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59922724fb8f42b89773d07020ee00d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08f9a17b3564e2aa17882a56aad3d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6134762a5b424ea7a0843b25ed392b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DtCleaner = DataCleaner()\n",
    "\n",
    "df_ta_cl = DtCleaner.remove_nonalpha_chars(df_ta, 'Review')\n",
    "df_ta_cl = DtCleaner.lower_casing(df_ta_cl, 'Review')\n",
    "df_ta_cl = DtCleaner.remove_stopwords(df_ta_cl, 'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f175873ae64a4f9d8f901435a12a97cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.remove_nonalpha_chars(scifi_df, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902d10ad7834bf496740a6f50601fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.lower_casing(df_scifi_cl, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d833e63930404d34a7bc828e682835db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.remove_stopwords(df_scifi_cl, \"Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into a training and test set\n",
    "# using set seed to allow replication\n",
    "np.random.seed(123)\n",
    "m = np.random.rand(len(df_ta_cl)) < 0.7\n",
    "n = np.random.rand(len(df_scifi_cl)) < 0.7\n",
    "df_ta_train = df_ta_cl[m]\n",
    "df_ta_test = df_ta_cl[~m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005026597042604"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if split was correct\n",
    "len(df_ta_train) / len(df_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006076"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ta_train = df_ta_train['Review'].str.cat(sep=', ')\n",
    "corpus_ta_test =  df_ta_test['Review'].str.cat(sep=', ')\n",
    "\n",
    "corpus_ta = corpus_ta_train + corpus_ta_test\n",
    "len(corpus_ta.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_scifi = df_scifi_cl[\"Text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f22f39f34d0>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "# FREQUENCIES OF WORDS IN CORPUS\n",
    "sorted_corpus_freqs_ta = sorted(Counter(corpus_ta.split(\" \")).items(), key=lambda x: x[1], reverse=True)\n",
    "plt.plot([(x[1]) for x in sorted_corpus_freqs_ta[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chating 4.984856007449369e-07\n",
      "pillowsattentive 4.984856007449369e-07\n",
      "staffnegatives 4.984856007449369e-07\n",
      "brandi 4.984856007449369e-07\n",
      "pattens 4.984856007449369e-07\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_corpus_freqs_ta[:5]:\n",
    "    print(i[0], i[1]/sum([i[1] for i in sorted_corpus_freqs_ta]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.lines.Line2D at 0x7f22f3d43150>],\n",
       " [('said', 76385),\n",
       "  ('one', 57263),\n",
       "  ('would', 46663),\n",
       "  ('could', 41425),\n",
       "  ('like', 36472),\n",
       "  ('time', 32907),\n",
       "  ('back', 32185),\n",
       "  ('man', 30097),\n",
       "  ('know', 28632),\n",
       "  ('get', 24516)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXyU5b338c8vO0kgJCQESQJJAEVQUAibC0GtiFWrVo9rlaMoUG2r1dZjzznP42l7ep5utmoXAaEKWuUoWsWVugKKAokimygJBEhYAgmELIRs1/PH3GhQlgBJ7kzm+3695pWZ677vyW+mxW+uZa4x5xwiIhLawvwuQERE/KcwEBERhYGIiCgMREQEhYGIiAARfhdwvJKTk11mZqbfZYiIBI38/PxdzrmUQx0L2jDIzMwkLy/P7zJERIKGmW063DENE4mIiMJAREQUBiIigsJARERQGIiICAoDERFBYSAiIoRYGNTWNzJjUSEfbSjzuxQRkQ4laD90djzMYNb7G+mXEs/o7B5+lyMi0mGEVM8gOiKcSedksaSwjJXFe/wuR0SkwwipMAC4fmQfusZEMH3hBr9LERHpMEIuDLrGRPK90X15ffU2inZV+12OiEiHEHJhAHDL2ZlEhIcxY7F6ByIiEKJh0LNrDFcNS2defjGllbV+lyMi4ruQDAOAyWOzqW9s4okPivwuRUTEdyEbBlnJcUwY3IsnP9pEZW293+WIiPgqZMMAYGpuPyprG5i7bIvfpYiI+Cqkw2BoRnfGZPdg1vsbqWto8rscERHfhHQYAEwd14/te2t5cUWJ36WIiPgm5MNg7IBkTj2pG9MXFtLU5PwuR0TEFyEfBmbG1NxsCndW89ZnO/wuR0TEFyEfBgCXnH4S6YldmL5IH0ITkdCkMAAiwsO4/dxs8jftZnlRud/liIi0O4WB55qcDJLiopj2XqHfpYiItDuFgadLVDgTx2Ty9rpSPt9e6Xc5IiLtSmHQzM1j+tIlMpzpi9Q7EJHQojBoJjEuimtHZDB/xVa27tnndzkiIu1GYfA1t52bhSPw9ZgiIqFCYfA16YmxfGdob55Ztpk9NXV+lyMi0i6OGgZmdoqZrWh222tmd5tZkpm9aWbrvZ+J3vlmZo+YWYGZrTSzYc2ea6J3/nozm9isfbiZrfKuecTMrG1ebstMyc2mpq6RJz/c5GcZIiLt5qhh4Jz73Dl3hnPuDGA4UAP8A7gfeNs5NwB423sMcDEwwLtNBh4FMLMk4AFgFDASeOBAgHjn3N7sugmt8uqO08Be3Rh3SgpPLCmitr7Rz1JERNrFsQ4TXQAUOuc2AZcDs7322cAV3v3LgTku4COgu5mdBFwEvOmcK3fO7QbeBCZ4x7o55z5yzjlgTrPn8s3U3H6UVdfxXH6x36WIiLS5Yw2D64BnvPupzrlt3v3tQKp3Pw1o/gUBxV7bkdqLD9Huq1FZSZyR0Z3HFm2goVHbW4tI59biMDCzKOA7wHNfP+b9Rd/mW36a2WQzyzOzvJ07d7b172Jqbj82l9fw+urtbfq7RET8diw9g4uBj51zB7b23OEN8eD9LPXaS4CMZtele21Hak8/RPs3OOdmOOdynHM5KSkpx1D68Rk/KJXs5DimLSwkkHciIp3TsYTB9Xw1RAQwHziwImgi8FKz9pu9VUWjgQpvOGkBMN7MEr2J4/HAAu/YXjMb7a0iurnZc/kqLMyYPDabNVv38n7BLr/LERFpMy0KAzOLAy4EXmjW/GvgQjNbD3zLewzwGrABKAAeA+4AcM6VA78Elnu3X3hteOfM9K4pBF4//pfUuq4clkbPrtFMX6jtrUWk84poyUnOuWqgx9faygisLvr6uQ648zDP8zfgb4dozwNOa0kt7S06Ipxbz8ni16+vY1VxBaenJ/hdkohIq9MnkFvghlF96BodwTRtYCcinZTCoAW6xURy4+i+vL5qG5vKqv0uR0Sk1SkMWujWszOJCAtjhr4aU0Q6IYVBC/XsFsN3h6XxXH4xOyv3+12OiEirUhgcg8ljs6lvbGL2kiK/SxERaVUKg2OQnRLPRYN6MefDIqr2N/hdjohIq1EYHKMpudnsrW1g7rLNfpciItJqFAbH6Mw+iYzKSmLm4o3UNWgDOxHpHBQGx2HquH5s31vL/E+3+l2KiEirUBgch3EnpzCwV1emLyykqUkb2IlI8FMYHIcD21uvL63inXWlR79ARKSDUxgcp0uGnERa9y5MW6gtKkQk+CkMjlNkeBi3nZtF3qbd5BWVH/0CEZEOTGFwAq4dkUFibCTTtL21iAQ5hcEJiI2K4OYxmbz12Q7W76j0uxwRkeOmMDhBE8/KJCYyjOnawE5EgpjC4AQlxUVxbU4GL60oYVvFPr/LERE5LgqDVnDbudk0OZi1eKPfpYiIHBeFQSvISIrl0iEn8cyyzVTU1PtdjojIMVMYtJIpY/tRXdfIU0s3+V2KiMgxUxi0kkG9u5F7cgqPf7CR2vpGv8sRETkmCoNWNCU3m11VdczLL/a7FBGRY6IwaEVjsnswND2BxxZvoFEb2IlIEFEYtKIDG9htKqvh9dXb/C5HRKTFFAatbPzgXmQlxzF94QacU+9ARIKDwqCVhYcZk8dms6qkgiWFZX6XIyLSIgqDNnDlmWkkx0dre2sRCRoKgzYQExnOredksnj9LlaXVPhdjojIUSkM2siNo/oSHx2h3oGIBAWFQRtJ6BLJjaP68NqqbWwuq/G7HBGRI2pRGJhZdzObZ2brzOwzMxtjZklm9qaZrfd+Jnrnmpk9YmYFZrbSzIY1e56J3vnrzWxis/bhZrbKu+YRM7PWf6nt79ZzsggPMx5brO2tRaRja2nP4GHgDefcQGAo8BlwP/C2c24A8Lb3GOBiYIB3mww8CmBmScADwChgJPDAgQDxzrm92XUTTuxldQyp3WK48sw0ns3bwq6q/X6XIyJyWEcNAzNLAMYCswCcc3XOuT3A5cBs77TZwBXe/cuBOS7gI6C7mZ0EXAS86Zwrd87tBt4EJnjHujnnPnKBhflzmj1X0Js8th91jU3MXlLkdykiIofVkp5BFrATeNzMPjGzmWYWB6Q65w58zHY7kOrdTwO2NLu+2Gs7UnvxIdq/wcwmm1memeXt3LmzBaX7r3/PeC48NZU5H26ien+D3+WIiBxSS8IgAhgGPOqcOxOo5qshIQC8v+jb/OO2zrkZzrkc51xOSkpKW/+6VjN1XD8q9tUzd/mWo58sIuKDloRBMVDsnFvqPZ5HIBx2eEM8eD9LveMlQEaz69O9tiO1px+ivdMY1ieRkVlJzFq8gfrGJr/LERH5hqOGgXNuO7DFzE7xmi4A1gLzgQMrgiYCL3n35wM3e6uKRgMV3nDSAmC8mSV6E8fjgQXesb1mNtpbRXRzs+fqNKbmZrO1opb5K7b6XYqIyDdEtPC8HwJ/N7MoYANwC4EgedbMJgGbgGu8c18Dvg0UADXeuTjnys3sl8By77xfOOfKvft3AE8AXYDXvVunct4pPTkltSvTFxVy5ZlphIV1itWzItJJWLDurJmTk+Py8vL8LuOYvPBxMfc8+ymzJuZwwampR79ARKQVmVm+cy7nUMf0CeR2dNnQ3qR178L0hfoQmoh0LAqDdhQZHsakc7JYVlRO/qbdfpcjIvIlhUE7u3ZEBgldIrWBnYh0KAqDdhYXHcHEMX15c+0O1m3f63c5IiKAwsAXE8/KJCkuijue+piKmnq/yxERURj4oUd8NNO+N5wtu2u44+l8fRBNRHynMPDJyKwk/ufK0/mgoIxfvLzW73JEJMS19ENn0gb+JSeDgtIqpi/aQP+e8Uw8K9PvkkQkRCkMfHbfhIEU7qzm5y+vISs5jrEnB88GfCLSeWiYyGfhYcZD153ByaldufPpjykorfS7JBEJQQqDDiA+OoKZE3OIjghj0uw8dlfX+V2SiIQYhUEHkZ4Yy/SbcthWUcvUp/Kpa9AKIxFpPwqDDmR430R+e9UQlm4s5/+8uJpg3URQRIKPJpA7mCvOTKOgtIo/v1vAgNR4bjs32++SRCQEKAw6oHsuPJnCnVX86rXPyEqO03bXItLmNEzUAYWFGQ9eM5TBvbvxo2c+4fPtWmEkIm1LYdBBxUZFMPPmEcRFRzBp9nJ2Ve33uyQR6cQUBh1Yr4QYZk7MYWflfqY+mc/+hka/SxKRTkph0MENSe/Og9cMJW/Tbn72wiqtMBKRNqEJ5CBw6ZDeFJZW88e3vmBAz658f1w/v0sSkU5GYRAkfnRBfwp2VvHbBevITonjosG9/C5JRDoRDRMFCTPjd1cPYUh6d+6eu4LVJRV+lyQinYjCIIjERIbz2E3D6R4bye1z8iitrPW7JBHpJBQGQaZntxgeuzmHPTX13D4nn9p6rTASkROnMAhCp6Ul8NB1Z/Dplj38dN5KrTASkROmMAhSFw3uxX0TTuHlT7fyp3cK/C5HRIKcVhMFse/n9qOgtIo/vPkF2SlxXDqkt98liUiQUs8giJkZ/++7p5PTN5F7n/2UT7fs8bskEQlSCoMgFx0RzrSbhpMcH83tc/LYXqEVRiJy7FoUBmZWZGarzGyFmeV5bUlm9qaZrfd+JnrtZmaPmFmBma00s2HNnmeid/56M5vYrH249/wF3rXW2i+0M0uOj2bWv+ZQvb+B2+YsZ1+dVhiJyLE5lp7Bec65M5xzOd7j+4G3nXMDgLe9xwAXAwO822TgUQiEB/AAMAoYCTxwIEC8c25vdt2E435FIWpgr2786YYzWbN1L/c8u4KmJq0wEpGWO5FhosuB2d792cAVzdrnuICPgO5mdhJwEfCmc67cObcbeBOY4B3r5pz7yAXWSM5p9lxyDM4fmMp/fPtUXl+9nT++9YXf5YhIEGlpGDjgn2aWb2aTvbZU59w27/524MDXcaUBW5pdW+y1Ham9+BDt32Bmk80sz8zydu7c2cLSQ8ukc7K4NieDP71TwEsrSvwuR0SCREuXlp7jnCsxs57Am2a2rvlB55wzszYfl3DOzQBmAOTk5Ggc5BDMjF9ecRpFZdX8dN5K0hNjGd438egXikhIa1HPwDlX4v0sBf5BYMx/hzfEg/ez1Du9BMhodnm613ak9vRDtMtxiooIY9r3hnNSQgxTnsyjeHeN3yWJSAd31DAwszgz63rgPjAeWA3MBw6sCJoIvOTdnw/c7K0qGg1UeMNJC4DxZpboTRyPBxZ4x/aa2WhvFdHNzZ5LjlNiXBSzJuawv6GJ22bnUb2/we+SRKQDa0nPIBV438w+BZYBrzrn3gB+DVxoZuuBb3mPAV4DNgAFwGPAHQDOuXLgl8By7/YLrw3vnJneNYXA6yf+0qR/z6785YZhrC+t4q65K2jUCiMROQwL1k3OcnJyXF5ent9lBIXZS4p4YP4apuRm87OLT/W7HBHxiZnlN/t4wEG0N1EIuHlMX9aXVjJ94Qb6p8TzLzkZR79IREKKtqMIAWbGA5cN5uz+Pfj3f6xi2cbyo18kIiFFYRAiIsPD+OsNw8lIjGXKk3lsLtMKIxH5isIghCTERjLrX0fQ5GDS7OVU1tb7XZKIdBAKgxCTlRzHozcOY+Ouan74zCdaYSQigMIgJJ3VP5mfXz6Y9z7fyX+/ulZfmykiWk0Uqm4c1ZeC0ioe/6CIgtIqHrhsMP17xvtdloj4RD2DEPaflwzigcsGsWLLHiY8tIhfvbpW8wgiIUphEMLCw4xbzs7i3Z+M46ph6cx8fyPnP7iQ5/OL9X0IIiFGYSAkx0fzm6uH8OIdZ9O7exfufe5Trp62hNUlFX6XJiLtRGEgXxqa0Z1/fP8sfnv1EDaX13DZn9/nZy+sory6zu/SRKSNKQzkIGFhxjU5Gbx97zhuOSuLZ/O2cN7v32POh0U0NDb5XZ6ItBGFgRxSQpdI/u9lg3j9rnMZ3Lsb//elNVz25w+0lYVIJ6UwkCM6ObUrf79tFH+9cRgVNXVcM/1D7pr7Cdsrav0uTURakcJAjsrM+PbpJ/HWvbn88Pz+vL56O+c/+B6PvlfI/oZGv8sTkVagMJAWi42K4N7xp/DWj3M5q18yv3ljHRMeWsy7n5ce/WIR6dAUBnLM+vSIZebEHJ64ZQQG3PL4cm6bvZxNZdV+lyYix0lhIMdt3Ck9eePusdx/8UCWFJZx4R8X8eA/P2dfnYaORIKNwkBOSFREGFNz+/HOveO4+LRe/OmdAi548D1eXblNG+CJBBGFgbSKXgkxPHzdmTw7ZQwJsVHc+fTH3PDYUr7YUel3aSLSAgoDaVUjs5J45Yfn8MvLB7N2214ufngxP395DRX7tAGeSEemMJBWFx5m3DQmk3d/Mo5rR2TwxJIiLnjwPZ5dvkUb4Il0UAoDaTNJcVH8z5Wn8/IPzqFvjzjue34lVz66hBVb9vhdmoh8jcJA2txpaQnMmzqGP1wzlK179nHFXz7g3+atZFfVfr9LExGPwkDahZnx3WHpvHNvLpPHZvP8x8Wc9/v3ePyDjdoAT6QDUBhIu+oaE8m/f/tU3rh7LGdkdOfnL6/lkkfeZ0nhLr9LEwlpCgPxRf+e8cy5dSTTbxpOdV0DNzy2lNvn5LFhZ5XfpYmEJIWB+MbMuGhwL966J5efXnQKHxaWMf6Pi/iv+Wv0hToi7UxhIL6LiQznzvP6f7kUdc6HReT+7l1mLNKuqCLtpcVhYGbhZvaJmb3iPc4ys6VmVmBm/2tmUV57tPe4wDue2ew5fua1f25mFzVrn+C1FZjZ/a338iSYpHSN5ldXns6Cu8eS0zeR/3ltHRc8uJBXVm7V1hYibexYegZ3AZ81e/wb4I/Ouf7AbmCS1z4J2O21/9E7DzMbBFwHDAYmAH/1AiYc+AtwMTAIuN47V0LUgNSuPH7LSJ6aNIr46Ah+8PQnfPfRJeRv2u13aSKdVovCwMzSgUuAmd5jA84H5nmnzAau8O5f7j3GO36Bd/7lwFzn3H7n3EagABjp3Qqccxucc3XAXO9cCXHnDEjm1R+dy2+vGkLJ7n1c9egS7vz7x2wuq/G7NJFOp6U9g4eA+4ADC8J7AHuccw3e42IgzbufBmwB8I5XeOd/2f61aw7X/g1mNtnM8swsb+fOnS0sXYJZeJhxzYgM3v3JOO66YADvrCvlW39YyK9eXUtFjfY7EmktRw0DM7sUKHXO5bdDPUfknJvhnMtxzuWkpKT4XY60o7joCH584cm899NxXH5Gb2a+v5Hc37/L4x9spK5BH1oTOVEt6RmcDXzHzIoIDOGcDzwMdDezCO+cdKDEu18CZAB4xxOAsubtX7vmcO0i35DaLYbf/ctQXvnhOQzu3Y2fv7yWix5axII12zXJLHICjhoGzrmfOefSnXOZBCaA33HO3Qi8C1ztnTYReMm7P997jHf8HRf4VzofuM5bbZQFDACWAcuBAd7qpCjvd8xvlVcnndbg3gk8NWkUj//rCMLDjClP5nPtjI9YWaxN8ESOx4l8zuDfgHvMrIDAnMAsr30W0MNrvwe4H8A5twZ4FlgLvAHc6Zxr9OYVfgAsILBa6VnvXJEjMjPOG9iTN+46l/++4jQKS6v4zp8/4O65n1CyZ5/f5YkEFQvWrnVOTo7Ly8vzuwzpQCpr63n0vUJmvr8RAyadk8X3x/Wja0yk36WJdAhmlu+cyznUMX0CWTqNrjGR3DdhIO/+JPB9zH99r5Dzfv8eT320STujihyFwkA6nbTuXXjoujN56c6zyU6O5z9fXM3FDy/m3XWlmmQWOQyFgXRaQzO6879TRjP9puHUNzZxyxPLuWnWMtZu3et3aSIdjsJAOrUDO6P+88e5PHDZIFZvreCSPy3mvnmfsmNvrd/liXQYmkCWkFJRU8+f313PE0uKiAgLY0puNpPHZhMbFXH0i0WCnCaQRTwJsZH8xyWDePuecZw/sCcPvbWecb97j2eXb6GxKTj/MBJpDQoDCUl9esTylxuH8fz3x5CW2IX7nl/JJY8s5u3PdigUJCRpmEhCnnOOV1Zu4zdvrKN49z7Sunfh+pEZXJOTQc9uMX6XJ9JqjjRMpDAQ8dQ1NPHPtdt5ZtlmPigoIyLM+Napqdwwqg/n9E8mLMz8LlHkhBwpDDRrJuKJigjj0iG9uXRIbzbuqmbuss08l1/MG2u2k5HUhetG9OGanAxSukb7XapIq1PPQOQI9jc0smDNDp5euomPNpQTEWaMH5zKDSP7cla/HuotSFDRMJFIKyjcWcXcZZuZl1/M7pp6+vaI5fqRfbh6eDrJ8eotSMenMBBpRbX1jSxYs52/L93Mso3lRIYb4wf34saRfRjTrweBb3kV6XgUBiJtpKC0kqeXbuH5j4up2FdPVnIc14/M4Kph6fRQb0E6GIWBSBurrW/k9dXbeHrpZpYX7SYqPIyLTuvFDSP7MDo7Sb0F6RAUBiLt6IsdlTy9dDMvfFzM3toGslPiuGFkH64alk5iXJTf5UkIUxiI+KC2vpFXV27j6WWbyd8U6C1cfHqgtzAyS70FaX8KAxGfrdu+l2eWbuaFT0qorG2gf894rh/Zh6uGpdE9Vr0FaR8KA5EOYl9dIy+v3MozyzbzyeY9gQ+6nX4S14/qQ07fRPUWpE0pDEQ6oLVb9/LMss28+EkJlfsbODk10Fv47pnpJMTqe5ul9SkMRDqwmroGXv50K08v3cynxRVER4RxWloCSXFR9IiLIulrtx5x0STFB47FRIb7Xb4EEYWBSJBYXVLBs3lbWL+jivLqOsqq69hdU3fYbbVjo8JJjI2iR3zzsIgiKS6apLhI76fXFh9F1+gIDUWFMG1UJxIkTktL4LS0hIPampoclbUNlFXv/zIgyr92K6uuY1fVfr7YXklZdR37G5oO+fyR4UZirBca8YHQOND7SGzWE+kRF0WP+GgSYyMVHiFCYSDSwYWFGQmxkSTERpKd0rJrauoaKKs6dGiUNwuVkt17KKuuo7K24ZDPk9otmpzMJEZmJpGTmcjAXt0I1+Z8nZLCQKQTio2KIDYpgoyk2BadX9fQxO6aOsqqAsNSZdV1lO6tZWVxBcuLynl15TYA4qMjGNY3kRF9ExmRlcQZGd01b9FJKAxEhKiIMFK7xZB6mG92K9mzj7yicpZtLCevaDcPvvkFEBh2Oi0tgRGZSYzITGJ430SS9CnroKQJZBE5Zntq6vh4826WbdxNXlE5K4srqGsMzFP07xnPiMxEcvomMTIrifTELpp36CC0mkhE2lRtfSOrSiq8nkM5eZt2fzkPcWDe4cDQkuYd/KPVRCLSpmIiw78cKoLACqgvSitZvrGc5UW7DzvvkJMZmHfoEqV5B78dtWdgZjHAIiCaQHjMc849YGZZwFygB5AP3OScqzOzaGAOMBwoA651zhV5z/UzYBLQCPzIObfAa58APAyEAzOdc78+WuHqGYgEl6/PO3y+oxI4eN4hxwsIzTu0jRMaJrLAYF+cc67KzCKB94G7gHuAF5xzc81sGvCpc+5RM7sDGOKcm2pm1wFXOueuNbNBwDPASKA38BZwsvdrvgAuBIqB5cD1zrm1R6pLYSAS3Cpq6snf7PUcNh5+3iEnM5E+SbGad2gFJzRM5AJpUeU9jPRuDjgfuMFrnw38F/AocLl3H2Ae8GcvUC4H5jrn9gMbzayAQDAAFDjnNnjFzvXOPWIYiEhwS4iN5PyBqZw/MBX4at5heVE5yzeW88rKbTyzbAsAcVHh9O8ZT/+eXRmQGs/JqfEM6NmVtO5dCNP8Q6to0ZyBmYUTGArqD/wFKAT2OOcOfFKlGEjz7qcBWwCccw1mVkFgKCkN+KjZ0za/ZsvX2kcdpo7JwGSAPn36tKR0EQkSB807jPtq3uHjTXv4YkclBaVVLF6/k+c/Lv7ymi6RgZAY0DOe/l5AnJwaT3pirCapj1GLwsA51wicYWbdgX8AA9u0qsPXMQOYAYFhIj9qEJH2ERZmDOzVjYG9uh3UXlFTT8HOSr7YUcX6HVWsL63kww1lvPBJyZfnREeE0S/F60GkdqV/z3hOTu1KnySFxOEc02oi59weM3sXGAN0N7MIr3eQDhz4X6IEyACKzSwCSCAwkXyg/YDm1xyuXUTkIAmxkQzvm8TwvkkHte+traegtIoCLyC+2FHF8qLdvLhi65fnREWEkZ0cx4DUrpzcM54BqYGhp749YokMD2vvl9KhHDUMzCwFqPeCoAuBid7fAO8CVxNYUTQReMm7ZL73+EPv+DvOOWdm84GnzewPBCaQBwDLAAMGeKuTSoDr+GouQkSkRbrFRDKsTyLD+iQe1F61v4HC0qovh5rWl1axYstuXv70q5CIDDeykw8MNQV6EQN6xtO3RxxREaEREi3pGZwEzPbmDcKAZ51zr5jZWmCumf038Akwyzt/FvCkN0FcTuA/7jjn1pjZswQmhhuAO73hJ8zsB8ACAktL/+acW9Nqr1BEQlp8dARDM7ozNKP7Qe01dQ0UllZ/2YsoKK1kdUkFr63axoFFlhFhRmZyHCenxjO4dwLX5GSQ0jXah1fR9vQJZBGRZmrrGwPDTaWB4abAvEQVRWXVRIaHcfXwdCafm01mcpzfpR4zfQJZRKSFYiLDD/m9Eht3VfPY4g3Myy/mmWWbufi0XkzN7ceQ9O6Heabgop6BiMgxKK2s5YkPinjyo01U1jZwVr8eTM3tx7kDkjv8B+O0UZ2ISCurrK1n7rItzHp/I9v31jLopG5Myc3mktNPIqKDrkxSGIiItJG6hiZeXFHC9IWFFO6sJj2xC7efm801ORkdbgM+hYGISBtranK8va6UaQsLyd+0m6S4KCaOyeTmMX1J7CAb7ykMRETa0fKicqa9V8jb60rpEhnOtSMyuO3cLNITW/Y1pG1FYSAi4oPPt1cyY9EGXlpRggO+M7Q3U3Kzv7HFRntRGIiI+Gjrnn3Men8jzyzbTE1dI+NOSWFqbj9GZSW16wokhYGISAewp6aOpz7axOMfFFFWXccZGd2ZmpvN+EG92mUrboWBiEgHUlvfyHP5xTy2aAOby2vITo5j8thsrhyWRnRE261AUhiIiHRAjU2O11dvY9rCQlaX7CWlazS3np3FjaP70C0mstV/n8JARKQDc87xQUEZ0xcVsnj9LrpGR3DD6D5MOjuLnt1iWktJa8sAAAOgSURBVO33KAxERILE6pIKpi0s5LVV24gIC+PKM9OYnJtNv5T4E35uhYGISJDZVBbYGO+5vGLqGpsYPyiVKbn9vvF9DcdCYSAiEqR2Ve1n9pIi5ny4iYp99YzMSmLOrSOJiTz2iWZtYS0iEqSS46O5d/wpTM3tx9zlW1i/o/K4guBoFAYiIkEgLjqCSedktdnzd8x9VkVEpF0pDERERGEgIiIKAxERQWEgIiIoDEREBIWBiIigMBAREYJ4Owoz2wlsOs7Lk4FdrVhOMNN7cTC9HwfT+/GVzvBe9HXOpRzqQNCGwYkws7zD7c8RavReHEzvx8H0fnyls78XGiYSERGFgYiIhG4YzPC7gA5E78XB9H4cTO/HVzr1exGScwYiInKwUO0ZiIhIMwoDEREJrTAwswlm9rmZFZjZ/X7X4yczyzCzd81srZmtMbO7/K7Jb2YWbmafmNkrftfiNzPrbmbzzGydmX1mZmP8rslPZvZj79/JajN7xsxi/K6ptYVMGJhZOPAX4GJgEHC9mQ3ytypfNQD3OucGAaOBO0P8/QC4C/jM7yI6iIeBN5xzA4GhhPD7YmZpwI+AHOfcaUA4cJ2/VbW+kAkDYCRQ4Jzb4JyrA+YCl/tck2+cc9uccx979ysJ/GNP87cq/5hZOnAJMNPvWvxmZgnAWGAWgHOuzjm3x9+qfBcBdDGzCCAW2OpzPa0ulMIgDdjS7HExIfwfv+bMLBM4E1jqbyW+egi4D2jyu5AOIAvYCTzuDZvNNLM4v4vyi3OuBPg9sBnYBlQ45/7pb1WtL5TCQA7BzOKB54G7nXN7/a7HD2Z2KVDqnMv3u5YOIgIYBjzqnDsTqAZCdo7NzBIJjCJkAb2BODP7nr9Vtb5QCoMSIKPZ43SvLWSZWSSBIPi7c+4Fv+vx0dnAd8ysiMDw4flm9pS/JfmqGCh2zh3oKc4jEA6h6lvARufcTudcPfACcJbPNbW6UAqD5cAAM8sysygCE0Dzfa7JN2ZmBMaEP3PO/cHvevzknPuZcy7dOZdJ4P8X7zjnOt1ffi3lnNsObDGzU7ymC4C1Ppbkt83AaDOL9f7dXEAnnFCP8LuA9uKcazCzHwALCKwG+Jtzbo3PZfnpbOAmYJWZrfDa/t0595qPNUnH8UPg794fThuAW3yuxzfOuaVmNg/4mMAqvE/ohFtTaDsKEREJqWEiERE5DIWBiIgoDERERGEgIiIoDEREBIWBiIigMBAREeD/A1xo71BjkgbjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_corpus_freqs_scifi = sorted(Counter(corpus_scifi.split(\" \")).items(), key=lambda x: x[1], reverse=True)\n",
    "plt.plot([(x[1]) for x in sorted_corpus_freqs_scifi[:10]]), sorted_corpus_freqs_scifi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164900"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_corpus_freqs_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7001007930355352"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are checking the ration of training to test sample again because we split the dataframe above, not the corpus\n",
    "# in theory, we could have sampled a lot of rows from the DF with long strings and obtained a training set which is more than\n",
    "# 70% of the corpus. highly unlikely given the size of the DF and the random sampling. So just to make sure we got this right.\n",
    "\n",
    "len(corpus_ta_train) / (len(corpus_ta_train) + len(corpus_ta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n voc train: 44016\n",
      "n voc test: 28326\n",
      "n voc combined: 52497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8480"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n voc train: \" + str(len(set(corpus_ta_train.split()))))\n",
    "print(\"n voc test: \" + str(len(set(corpus_ta_test.split()))))\n",
    "\n",
    "print(\"n voc combined: \" + str(len(set((corpus_ta_train + corpus_ta_test).split()))))\n",
    "\n",
    "len(set(corpus_ta_test.split()).difference(set(corpus_ta_train.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vectorize_layer = TextVectorization(\\n    standardize=custom_standardization,\\n    max_tokens=vocab_size,\\n    output_mode='int',\\n    output_sequence_length=sequence_length)\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "\"\"\"vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\"\"\"\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "#text_ds = train_ds.map(lambda x, y: x)\n",
    "#vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary encoding\n",
    "Encoding our vocabulary. We are encoding the full corpus, as suggested in the exercise forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ta = set(corpus_ta.split())\n",
    "vocab_ta_size = len(vocab_ta)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab_ta)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52497,\n",
       " {'spacoious': 0,\n",
       "  'whales': 1,\n",
       "  'intently': 2,\n",
       "  'laundered': 3,\n",
       "  'kimberley': 4,\n",
       "  'predictions': 5,\n",
       "  'chacun': 6,\n",
       "  'lieu': 7,\n",
       "  'kon': 8,\n",
       "  'inexpensive': 9,\n",
       "  'eh': 10,\n",
       "  'bascially': 11,\n",
       "  'liabeny': 12,\n",
       "  'trebled': 13,\n",
       "  'innocent': 14,\n",
       "  'sleva,': 15,\n",
       "  'homour': 16,\n",
       "  'poolwe': 17,\n",
       "  'aquadic': 18,\n",
       "  'thinklots': 19,\n",
       "  'academie': 20,\n",
       "  'masterful': 21,\n",
       "  'eaterys': 22,\n",
       "  'jjust': 23,\n",
       "  'vactioning': 24,\n",
       "  'larges': 25,\n",
       "  'uniformity': 26,\n",
       "  'jaywalkers': 27,\n",
       "  'equpped': 28,\n",
       "  'privil': 29,\n",
       "  'cinemax': 30,\n",
       "  'roped': 31,\n",
       "  'tana': 32,\n",
       "  'barrel': 33,\n",
       "  'woodpeckers': 34,\n",
       "  'dreadul': 35,\n",
       "  'therendezvous,': 36,\n",
       "  'feroni': 37,\n",
       "  'hoku': 38,\n",
       "  'resigned': 39,\n",
       "  'forte': 40,\n",
       "  'attained': 41,\n",
       "  'trael': 42,\n",
       "  'mkt': 43,\n",
       "  'conuco': 44,\n",
       "  'argentinan': 45,\n",
       "  'jucies': 46,\n",
       "  'deejayed': 47,\n",
       "  'cavetelli': 48,\n",
       "  'guaranteeable': 49,\n",
       "  'hiltonhonors': 50,\n",
       "  'harry': 51,\n",
       "  'restaurantso': 52,\n",
       "  'fruitstands': 53,\n",
       "  'cancel': 54,\n",
       "  'tippingwe': 55,\n",
       "  'modigliani,': 56,\n",
       "  'reactivate': 57,\n",
       "  'peoria': 58,\n",
       "  'bats': 59,\n",
       "  'polished': 60,\n",
       "  'triomphe': 61,\n",
       "  'whack': 62,\n",
       "  'honorary': 63,\n",
       "  'tobu': 64,\n",
       "  'armstrong': 65,\n",
       "  'tappanayki': 66,\n",
       "  'policed': 67,\n",
       "  'pommel': 68,\n",
       "  'buss': 69,\n",
       "  'thingit': 70,\n",
       "  'disappointmented': 71,\n",
       "  'hooray': 72,\n",
       "  'nhotel': 73,\n",
       "  'camery': 74,\n",
       "  'poster': 75,\n",
       "  'strabe': 76,\n",
       "  'liking': 77,\n",
       "  'wreaked': 78,\n",
       "  'codominium': 79,\n",
       "  'unfortuantely': 80,\n",
       "  'moira': 81,\n",
       "  'cages,': 82,\n",
       "  'rodizio': 83,\n",
       "  'harmony': 84,\n",
       "  'juluca': 85,\n",
       "  'nathalie': 86,\n",
       "  'banyan': 87,\n",
       "  'carting': 88,\n",
       "  'neiborgh': 89,\n",
       "  'speaks': 90,\n",
       "  'es': 91,\n",
       "  'shody': 92,\n",
       "  'slab': 93,\n",
       "  'dig': 94,\n",
       "  'brodge': 95,\n",
       "  'hades': 96,\n",
       "  'corn': 97,\n",
       "  'panini': 98,\n",
       "  'contrasting': 99,\n",
       "  'fouth': 100,\n",
       "  'fqf': 101,\n",
       "  'atttitude': 102,\n",
       "  'surved': 103,\n",
       "  'garnered': 104,\n",
       "  'roppongi,': 105,\n",
       "  'checkin': 106,\n",
       "  'shades': 107,\n",
       "  'jones': 108,\n",
       "  'manintained': 109,\n",
       "  'canopys': 110,\n",
       "  'beerthe': 111,\n",
       "  'thoroughfares': 112,\n",
       "  'short': 113,\n",
       "  'dangerously': 114,\n",
       "  'nicked': 115,\n",
       "  'amsterdan': 116,\n",
       "  'argenteria': 117,\n",
       "  'cassino': 118,\n",
       "  'sublevel': 119,\n",
       "  'omote': 120,\n",
       "  'katerina': 121,\n",
       "  'gu': 122,\n",
       "  'features': 123,\n",
       "  'emuanel': 124,\n",
       "  'sleezy': 125,\n",
       "  'sc': 126,\n",
       "  'straight,': 127,\n",
       "  'imprisioned': 128,\n",
       "  'isolate': 129,\n",
       "  'hlotel': 130,\n",
       "  'champaine': 131,\n",
       "  'grief': 132,\n",
       "  'stab': 133,\n",
       "  'roomsslow': 134,\n",
       "  'holly': 135,\n",
       "  'vacations,': 136,\n",
       "  'strat': 137,\n",
       "  'chaicago': 138,\n",
       "  'belatedly': 139,\n",
       "  'londoners': 140,\n",
       "  'toget': 141,\n",
       "  'atractitions': 142,\n",
       "  'ram': 143,\n",
       "  'planne': 144,\n",
       "  'circus': 145,\n",
       "  'bugbites': 146,\n",
       "  'outsourcing': 147,\n",
       "  'repellent': 148,\n",
       "  'coventry': 149,\n",
       "  'soames': 150,\n",
       "  'toiletires': 151,\n",
       "  'channes': 152,\n",
       "  'prada': 153,\n",
       "  'digest,': 154,\n",
       "  'nightclub': 155,\n",
       "  'someof': 156,\n",
       "  'conversing': 157,\n",
       "  'ought': 158,\n",
       "  'claustrophic': 159,\n",
       "  'amazes': 160,\n",
       "  'waterfalls': 161,\n",
       "  'collegues': 162,\n",
       "  'aload': 163,\n",
       "  'indoors': 164,\n",
       "  'langley': 165,\n",
       "  'prize': 166,\n",
       "  'yellowing': 167,\n",
       "  'falter': 168,\n",
       "  'question,': 169,\n",
       "  'headquarters': 170,\n",
       "  'athletic': 171,\n",
       "  'stolys': 172,\n",
       "  'relatative': 173,\n",
       "  'spanger': 174,\n",
       "  'moreish': 175,\n",
       "  'cappuchino': 176,\n",
       "  'leidseplein': 177,\n",
       "  'clobbered': 178,\n",
       "  'extravagent': 179,\n",
       "  'hitel': 180,\n",
       "  'delighful': 181,\n",
       "  'jans': 182,\n",
       "  'vraiment': 183,\n",
       "  'congratulated': 184,\n",
       "  'hansen,': 185,\n",
       "  'odour': 186,\n",
       "  'multistory': 187,\n",
       "  'shiskabob': 188,\n",
       "  'carlton,': 189,\n",
       "  'birch': 190,\n",
       "  'scrunchies': 191,\n",
       "  'fedora': 192,\n",
       "  'almost': 193,\n",
       "  'mimimalist': 194,\n",
       "  'sleepless': 195,\n",
       "  'joggers': 196,\n",
       "  'orphanage': 197,\n",
       "  'plusheramas,': 198,\n",
       "  'anxiously': 199,\n",
       "  'hispanola': 200,\n",
       "  'sqm': 201,\n",
       "  'freind': 202,\n",
       "  'scone': 203,\n",
       "  'perfer': 204,\n",
       "  'speaking': 205,\n",
       "  'cardinal': 206,\n",
       "  'foursomes': 207,\n",
       "  'sign,': 208,\n",
       "  'recessed': 209,\n",
       "  'prblem': 210,\n",
       "  'poaching': 211,\n",
       "  'restauranttwice': 212,\n",
       "  'neutralizer': 213,\n",
       "  'gonzalez': 214,\n",
       "  'bluff': 215,\n",
       "  'located,': 216,\n",
       "  'eliminates': 217,\n",
       "  'inefficiency': 218,\n",
       "  'especiallyat': 219,\n",
       "  'boarding': 220,\n",
       "  'comical': 221,\n",
       "  'hoe': 222,\n",
       "  'commentaries': 223,\n",
       "  'bikers,': 224,\n",
       "  'solidly': 225,\n",
       "  'dwellings': 226,\n",
       "  'courgette': 227,\n",
       "  'quietly': 228,\n",
       "  'handily': 229,\n",
       "  'beopen': 230,\n",
       "  'eternally': 231,\n",
       "  'monterosso': 232,\n",
       "  'flovor': 233,\n",
       "  'digna': 234,\n",
       "  'vores': 235,\n",
       "  'occasionally': 236,\n",
       "  'capachinos': 237,\n",
       "  'backdrop': 238,\n",
       "  'rigorous': 239,\n",
       "  'pillowtop': 240,\n",
       "  'afther': 241,\n",
       "  'conveniently': 242,\n",
       "  'blow': 243,\n",
       "  'typicaly': 244,\n",
       "  'bbaye': 245,\n",
       "  'colloquially': 246,\n",
       "  'smelt': 247,\n",
       "  'octubre': 248,\n",
       "  'chris,': 249,\n",
       "  'salamander': 250,\n",
       "  'unfailingly': 251,\n",
       "  'foodpoisoning': 252,\n",
       "  'cribe': 253,\n",
       "  'uncaring': 254,\n",
       "  'compensatory': 255,\n",
       "  'op': 256,\n",
       "  'beetelnut': 257,\n",
       "  'reset': 258,\n",
       "  'springy': 259,\n",
       "  'eyebrows': 260,\n",
       "  'whattaya': 261,\n",
       "  'mlk': 262,\n",
       "  'minibus': 263,\n",
       "  'shrink': 264,\n",
       "  'greenwich': 265,\n",
       "  'path': 266,\n",
       "  'satisified': 267,\n",
       "  'bankok': 268,\n",
       "  'prime,': 269,\n",
       "  'outcasts': 270,\n",
       "  'wkly': 271,\n",
       "  'rep,': 272,\n",
       "  'uwajimaya': 273,\n",
       "  'bratwurst': 274,\n",
       "  'carraibe': 275,\n",
       "  'sharon': 276,\n",
       "  'westernised': 277,\n",
       "  'price': 278,\n",
       "  'donnabritish': 279,\n",
       "  'hung': 280,\n",
       "  'champaign': 281,\n",
       "  'turberon': 282,\n",
       "  'young': 283,\n",
       "  'hanna': 284,\n",
       "  'forging': 285,\n",
       "  'glacial': 286,\n",
       "  'husks': 287,\n",
       "  'liver': 288,\n",
       "  'himitsuchinese': 289,\n",
       "  'hypnotizing': 290,\n",
       "  'thank,': 291,\n",
       "  'arene': 292,\n",
       "  'perfectely': 293,\n",
       "  'manhattan,': 294,\n",
       "  'heavely': 295,\n",
       "  'bejing,': 296,\n",
       "  'oaisis': 297,\n",
       "  'els': 298,\n",
       "  'chilren': 299,\n",
       "  'famed': 300,\n",
       "  'formality': 301,\n",
       "  'noble': 302,\n",
       "  'pullout': 303,\n",
       "  'anywayfirst': 304,\n",
       "  'des': 305,\n",
       "  'cage': 306,\n",
       "  'salami': 307,\n",
       "  'returnt': 308,\n",
       "  'aparthotel': 309,\n",
       "  'cancelled': 310,\n",
       "  'agresive': 311,\n",
       "  'porch': 312,\n",
       "  'truck': 313,\n",
       "  'adjusts': 314,\n",
       "  'physically': 315,\n",
       "  'cours': 316,\n",
       "  'acct': 317,\n",
       "  'machetes': 318,\n",
       "  'vweights': 319,\n",
       "  'maintinance': 320,\n",
       "  'listen,': 321,\n",
       "  'colorful': 322,\n",
       "  'visually': 323,\n",
       "  'negatives,': 324,\n",
       "  'weighted': 325,\n",
       "  'days': 326,\n",
       "  'highlight,': 327,\n",
       "  'maidseemed': 328,\n",
       "  'catedral': 329,\n",
       "  'pillows,': 330,\n",
       "  'sycophantic': 331,\n",
       "  'fax': 332,\n",
       "  'surprice': 333,\n",
       "  'aday': 334,\n",
       "  'subsequent': 335,\n",
       "  'pricetravel': 336,\n",
       "  'accomodatiing': 337,\n",
       "  'utilized': 338,\n",
       "  'properly': 339,\n",
       "  'amont': 340,\n",
       "  'clackity': 341,\n",
       "  'choose,': 342,\n",
       "  'snorkerling': 343,\n",
       "  'franciscomy': 344,\n",
       "  'pation': 345,\n",
       "  'retiral': 346,\n",
       "  'ikea': 347,\n",
       "  'iterally': 348,\n",
       "  'koppel': 349,\n",
       "  'palm': 350,\n",
       "  'chiming': 351,\n",
       "  'colada': 352,\n",
       "  'cabride': 353,\n",
       "  'fittingly': 354,\n",
       "  'anomilie': 355,\n",
       "  'natured': 356,\n",
       "  'hurt': 357,\n",
       "  'excelllence': 358,\n",
       "  'bloomenmarkt': 359,\n",
       "  'approcach': 360,\n",
       "  'ahem': 361,\n",
       "  'zipline': 362,\n",
       "  'jeffhanman': 363,\n",
       "  'iberville': 364,\n",
       "  'conflicted': 365,\n",
       "  'eurostars': 366,\n",
       "  'vandalism': 367,\n",
       "  'diamondhead': 368,\n",
       "  'breed': 369,\n",
       "  'repect': 370,\n",
       "  'optheres': 371,\n",
       "  'decorated': 372,\n",
       "  'psyched': 373,\n",
       "  'gameboy': 374,\n",
       "  'georgian': 375,\n",
       "  'mines': 376,\n",
       "  'hooney': 377,\n",
       "  'ampitheatre': 378,\n",
       "  'worrth': 379,\n",
       "  'natour': 380,\n",
       "  'girlfirend': 381,\n",
       "  'byt': 382,\n",
       "  'calculated': 383,\n",
       "  'zapata': 384,\n",
       "  'hunched': 385,\n",
       "  'lime': 386,\n",
       "  'fuente': 387,\n",
       "  'bristles': 388,\n",
       "  'swarmed': 389,\n",
       "  'repalced': 390,\n",
       "  'cathay': 391,\n",
       "  'fritters': 392,\n",
       "  'tin': 393,\n",
       "  'servicethe': 394,\n",
       "  'highschool': 395,\n",
       "  'fog': 396,\n",
       "  'workdesk': 397,\n",
       "  'oreos': 398,\n",
       "  'compound': 399,\n",
       "  'oscars': 400,\n",
       "  'persuing': 401,\n",
       "  'interperter': 402,\n",
       "  'sedan,': 403,\n",
       "  'quallity': 404,\n",
       "  'nferior': 405,\n",
       "  'soulton': 406,\n",
       "  'marlene,': 407,\n",
       "  'thyself': 408,\n",
       "  'ishtar': 409,\n",
       "  'work,': 410,\n",
       "  'giuliani': 411,\n",
       "  'relunctantly': 412,\n",
       "  'curteous,': 413,\n",
       "  'onthe': 414,\n",
       "  'inconvienced': 415,\n",
       "  'stripkarten': 416,\n",
       "  'ardener': 417,\n",
       "  'damarios': 418,\n",
       "  'frappaccino': 419,\n",
       "  'doornob': 420,\n",
       "  'heroes': 421,\n",
       "  'jellies': 422,\n",
       "  'indianapolis': 423,\n",
       "  'ornage': 424,\n",
       "  'njorth': 425,\n",
       "  'buya': 426,\n",
       "  'philosophers': 427,\n",
       "  'joint': 428,\n",
       "  'lg': 429,\n",
       "  'inacceptable': 430,\n",
       "  'llet': 431,\n",
       "  'repellant': 432,\n",
       "  'trashcans': 433,\n",
       "  'ames': 434,\n",
       "  'patch': 435,\n",
       "  'sundeck': 436,\n",
       "  'gorgeous': 437,\n",
       "  'blemishes': 438,\n",
       "  'iterate': 439,\n",
       "  'velvet': 440,\n",
       "  'dramatic': 441,\n",
       "  'stepper': 442,\n",
       "  'stephanies': 443,\n",
       "  'stoic': 444,\n",
       "  'busy': 445,\n",
       "  'sucked': 446,\n",
       "  'ually': 447,\n",
       "  'fasinating,': 448,\n",
       "  'veyr': 449,\n",
       "  'jacques': 450,\n",
       "  'stealth': 451,\n",
       "  'benvenuto': 452,\n",
       "  'functioned': 453,\n",
       "  'kiko': 454,\n",
       "  'caramerlized': 455,\n",
       "  'bodied': 456,\n",
       "  'vinegar,': 457,\n",
       "  'generation,': 458,\n",
       "  'agents': 459,\n",
       "  'curving': 460,\n",
       "  'vein': 461,\n",
       "  'bismo': 462,\n",
       "  'rubbery': 463,\n",
       "  'bowling': 464,\n",
       "  'cosier': 465,\n",
       "  'crates': 466,\n",
       "  'eneded': 467,\n",
       "  'bloating': 468,\n",
       "  'lik': 469,\n",
       "  'thoughdiscogood': 470,\n",
       "  'coathooks': 471,\n",
       "  'united': 472,\n",
       "  'espresso': 473,\n",
       "  'mgd': 474,\n",
       "  'duster': 475,\n",
       "  'hard': 476,\n",
       "  'moody': 477,\n",
       "  'ithe': 478,\n",
       "  'chocolata': 479,\n",
       "  'archaic': 480,\n",
       "  'unprofessional': 481,\n",
       "  'tack': 482,\n",
       "  'ragged': 483,\n",
       "  'hors': 484,\n",
       "  'report,': 485,\n",
       "  'richlieu': 486,\n",
       "  'pommes': 487,\n",
       "  'house,': 488,\n",
       "  'helped,': 489,\n",
       "  'stealers': 490,\n",
       "  'trobadero': 491,\n",
       "  'hecomes': 492,\n",
       "  'lori': 493,\n",
       "  'spledid': 494,\n",
       "  'yuppy': 495,\n",
       "  'ordinaryily': 496,\n",
       "  'uniformly': 497,\n",
       "  'tress': 498,\n",
       "  'mason': 499,\n",
       "  'smart': 500,\n",
       "  'jist': 501,\n",
       "  'aggress': 502,\n",
       "  'vegetal': 503,\n",
       "  'rainy': 504,\n",
       "  'inflexible': 505,\n",
       "  'salmon': 506,\n",
       "  'horded': 507,\n",
       "  'danielle': 508,\n",
       "  'occidental,': 509,\n",
       "  'viewless': 510,\n",
       "  'sterile': 511,\n",
       "  'alexis,': 512,\n",
       "  'heights': 513,\n",
       "  'tsui,': 514,\n",
       "  'hurling': 515,\n",
       "  'bistecca': 516,\n",
       "  'goot': 517,\n",
       "  'idol': 518,\n",
       "  'plusin': 519,\n",
       "  'difficultly': 520,\n",
       "  'imperfect': 521,\n",
       "  'freaky': 522,\n",
       "  'salespeople': 523,\n",
       "  'fruitshake': 524,\n",
       "  'ktr': 525,\n",
       "  'dominicanand': 526,\n",
       "  'thijs': 527,\n",
       "  'tapatio': 528,\n",
       "  'mom': 529,\n",
       "  'cambs': 530,\n",
       "  'excvellent': 531,\n",
       "  'housebowling': 532,\n",
       "  'sheraton,': 533,\n",
       "  'peolple': 534,\n",
       "  'domnican': 535,\n",
       "  'usd,': 536,\n",
       "  'ca,': 537,\n",
       "  'stinked': 538,\n",
       "  'architects': 539,\n",
       "  'prabably': 540,\n",
       "  'arcelo': 541,\n",
       "  'bottledwater': 542,\n",
       "  'spent,': 543,\n",
       "  'ilihani': 544,\n",
       "  'bben': 545,\n",
       "  'nurtured': 546,\n",
       "  'none': 547,\n",
       "  'squeeze': 548,\n",
       "  'posting,': 549,\n",
       "  'leve': 550,\n",
       "  'reflective': 551,\n",
       "  'peppered': 552,\n",
       "  'allocated': 553,\n",
       "  'replica': 554,\n",
       "  'fajita': 555,\n",
       "  'actuations,': 556,\n",
       "  'attact': 557,\n",
       "  'quietshuttel': 558,\n",
       "  'transparent': 559,\n",
       "  'harder,': 560,\n",
       "  'bargaining': 561,\n",
       "  'atend': 562,\n",
       "  'countdown': 563,\n",
       "  'fresher': 564,\n",
       "  'samatha': 565,\n",
       "  'portrayals': 566,\n",
       "  'seeweed': 567,\n",
       "  'supermercado': 568,\n",
       "  'adams': 569,\n",
       "  'extemely': 570,\n",
       "  'solar': 571,\n",
       "  'akward': 572,\n",
       "  'pointsbeautiful': 573,\n",
       "  'pratical': 574,\n",
       "  'contacted': 575,\n",
       "  'unhelpfully': 576,\n",
       "  'twisting': 577,\n",
       "  'pontevecchio': 578,\n",
       "  'shortened': 579,\n",
       "  'goodness': 580,\n",
       "  'moms': 581,\n",
       "  'cinco': 582,\n",
       "  'rousing': 583,\n",
       "  'requesting': 584,\n",
       "  'blandness': 585,\n",
       "  'dictate': 586,\n",
       "  'hilo': 587,\n",
       "  'xian': 588,\n",
       "  'employes': 589,\n",
       "  'bangle': 590,\n",
       "  'infront': 591,\n",
       "  'rematch': 592,\n",
       "  'preceiding': 593,\n",
       "  'unwarrented': 594,\n",
       "  'glancing': 595,\n",
       "  'crackerbox': 596,\n",
       "  'pretends,': 597,\n",
       "  'connnection': 598,\n",
       "  'merveillous': 599,\n",
       "  'hardwork': 600,\n",
       "  'localsrestaurants': 601,\n",
       "  'daugther': 602,\n",
       "  'fron': 603,\n",
       "  'meeting': 604,\n",
       "  'teach': 605,\n",
       "  'officiousness': 606,\n",
       "  'extremeley': 607,\n",
       "  'gravina': 608,\n",
       "  'uncomfertable': 609,\n",
       "  'evry': 610,\n",
       "  'unsung': 611,\n",
       "  'walkingaround': 612,\n",
       "  'pointer': 613,\n",
       "  'inferior,': 614,\n",
       "  'customarily': 615,\n",
       "  'rethink': 616,\n",
       "  'italie': 617,\n",
       "  'pasaig': 618,\n",
       "  'scharm': 619,\n",
       "  'zooming': 620,\n",
       "  'dormitories': 621,\n",
       "  'staffare': 622,\n",
       "  'unlike': 623,\n",
       "  'ritzy': 624,\n",
       "  'orchestra': 625,\n",
       "  'appointments': 626,\n",
       "  'pornography': 627,\n",
       "  'lotion,': 628,\n",
       "  'controller': 629,\n",
       "  'fastest': 630,\n",
       "  'frigid': 631,\n",
       "  'realize': 632,\n",
       "  'becky': 633,\n",
       "  'siu': 634,\n",
       "  'otv': 635,\n",
       "  'instructional': 636,\n",
       "  'fraudulent': 637,\n",
       "  'quilts': 638,\n",
       "  'dewey': 639,\n",
       "  'within': 640,\n",
       "  'arab': 641,\n",
       "  'jane,': 642,\n",
       "  'snot': 643,\n",
       "  'unvbeleiveable': 644,\n",
       "  'promply': 645,\n",
       "  'gunawan': 646,\n",
       "  'hernandez': 647,\n",
       "  'chalant': 648,\n",
       "  'commend': 649,\n",
       "  'beachy': 650,\n",
       "  'compromise,': 651,\n",
       "  'timed': 652,\n",
       "  'wellorganised': 653,\n",
       "  'eat,': 654,\n",
       "  'sloan': 655,\n",
       "  'establishments': 656,\n",
       "  'bluntly': 657,\n",
       "  'repel': 658,\n",
       "  'themos': 659,\n",
       "  'complainedmy': 660,\n",
       "  'masiel': 661,\n",
       "  'monoprix': 662,\n",
       "  'bombsite': 663,\n",
       "  'wagenstraat': 664,\n",
       "  'miles': 665,\n",
       "  'lunatic': 666,\n",
       "  'alsoi': 667,\n",
       "  'staffminus': 668,\n",
       "  'plantains': 669,\n",
       "  'formula': 670,\n",
       "  'laying': 671,\n",
       "  'anther': 672,\n",
       "  'iffy': 673,\n",
       "  'concluded': 674,\n",
       "  'misery': 675,\n",
       "  'dominicians': 676,\n",
       "  'cappuccini': 677,\n",
       "  'unintentionally': 678,\n",
       "  'look,': 679,\n",
       "  'paladium': 680,\n",
       "  'restrauant': 681,\n",
       "  'bedcoverings': 682,\n",
       "  'freshed': 683,\n",
       "  'regret,': 684,\n",
       "  'exaggerated': 685,\n",
       "  'accompanying': 686,\n",
       "  'stair': 687,\n",
       "  'list,': 688,\n",
       "  'gamble,': 689,\n",
       "  'airpot': 690,\n",
       "  'kareoke': 691,\n",
       "  'salish': 692,\n",
       "  'taps': 693,\n",
       "  'hiddeous': 694,\n",
       "  'genteman': 695,\n",
       "  'hibachi': 696,\n",
       "  'loans': 697,\n",
       "  'barservice': 698,\n",
       "  'certinaly': 699,\n",
       "  'woe': 700,\n",
       "  'slices': 701,\n",
       "  'salvaged': 702,\n",
       "  'monies': 703,\n",
       "  'bed,': 704,\n",
       "  'favourably': 705,\n",
       "  'honsetly': 706,\n",
       "  'volker': 707,\n",
       "  'soooo': 708,\n",
       "  'recharger': 709,\n",
       "  'lovers,': 710,\n",
       "  'experimental': 711,\n",
       "  'motorists': 712,\n",
       "  'averageplouf': 713,\n",
       "  'cashier': 714,\n",
       "  'medical': 715,\n",
       "  'jerk': 716,\n",
       "  'cardiac': 717,\n",
       "  'meseums': 718,\n",
       "  'metres': 719,\n",
       "  'standardsthe': 720,\n",
       "  'apex': 721,\n",
       "  'recharged': 722,\n",
       "  'wass': 723,\n",
       "  'brilliantly': 724,\n",
       "  'tiiiiiiiny': 725,\n",
       "  'excuses': 726,\n",
       "  'metal,': 727,\n",
       "  'spotaround': 728,\n",
       "  'lti': 729,\n",
       "  'wot': 730,\n",
       "  'breitschield': 731,\n",
       "  'maruno': 732,\n",
       "  'ramsay,': 733,\n",
       "  'airy': 734,\n",
       "  'capris': 735,\n",
       "  'lateral': 736,\n",
       "  'indicated': 737,\n",
       "  'trotts': 738,\n",
       "  'dispensers': 739,\n",
       "  'boyfriend': 740,\n",
       "  'elaborated': 741,\n",
       "  'dahlia': 742,\n",
       "  'roomsexcellent': 743,\n",
       "  'shing': 744,\n",
       "  'warnings': 745,\n",
       "  'audition': 746,\n",
       "  'nighty': 747,\n",
       "  'gardening': 748,\n",
       "  'exists': 749,\n",
       "  'proms': 750,\n",
       "  'duardo': 751,\n",
       "  'luxemburg': 752,\n",
       "  'ataxi': 753,\n",
       "  'town,': 754,\n",
       "  'recapture': 755,\n",
       "  'wif': 756,\n",
       "  'dolomites': 757,\n",
       "  'hunan': 758,\n",
       "  'lovers': 759,\n",
       "  'franceschi': 760,\n",
       "  'verify': 761,\n",
       "  'sayan': 762,\n",
       "  'hastled': 763,\n",
       "  'peripheral': 764,\n",
       "  'prickly': 765,\n",
       "  'alexandria': 766,\n",
       "  'hilariously': 767,\n",
       "  'toru': 768,\n",
       "  'uggage': 769,\n",
       "  'behing': 770,\n",
       "  'unsmiling': 771,\n",
       "  'chauffer': 772,\n",
       "  'woryy': 773,\n",
       "  'skills': 774,\n",
       "  'nok': 775,\n",
       "  'safedeposit': 776,\n",
       "  'tine': 777,\n",
       "  'neighbourhoody': 778,\n",
       "  'best': 779,\n",
       "  'caps': 780,\n",
       "  'miniclub': 781,\n",
       "  'iad': 782,\n",
       "  'median': 783,\n",
       "  'revisits': 784,\n",
       "  'fauneil': 785,\n",
       "  'smarten': 786,\n",
       "  'enviroment': 787,\n",
       "  'fortuanately': 788,\n",
       "  'uptempo': 789,\n",
       "  'insurance': 790,\n",
       "  'etait': 791,\n",
       "  'interuptions': 792,\n",
       "  'liked': 793,\n",
       "  'sparkeling': 794,\n",
       "  'wes': 795,\n",
       "  'adjustments': 796,\n",
       "  'wouldlove': 797,\n",
       "  'dncing': 798,\n",
       "  'eddie': 799,\n",
       "  'thugsi': 800,\n",
       "  'nights,': 801,\n",
       "  'hypothosized': 802,\n",
       "  'treats': 803,\n",
       "  'anhalter': 804,\n",
       "  'award': 805,\n",
       "  'sprinklers': 806,\n",
       "  'otherwords': 807,\n",
       "  'jostled': 808,\n",
       "  'remaining': 809,\n",
       "  'legs': 810,\n",
       "  'teetime': 811,\n",
       "  'rear,': 812,\n",
       "  'sometime': 813,\n",
       "  'pools': 814,\n",
       "  'bedding': 815,\n",
       "  'original': 816,\n",
       "  'swear': 817,\n",
       "  'drowns': 818,\n",
       "  'matrass': 819,\n",
       "  'overheadad': 820,\n",
       "  'talking,': 821,\n",
       "  'represents,': 822,\n",
       "  'harrowing': 823,\n",
       "  'lindor': 824,\n",
       "  'raking': 825,\n",
       "  'disorganisation': 826,\n",
       "  'lucked': 827,\n",
       "  'maitenence': 828,\n",
       "  'grammys': 829,\n",
       "  'brunoise': 830,\n",
       "  'enrico': 831,\n",
       "  'bugamvillia': 832,\n",
       "  'direcktly': 833,\n",
       "  'addition,': 834,\n",
       "  'vanderbilt': 835,\n",
       "  'farts': 836,\n",
       "  'fresc': 837,\n",
       "  'pharmacy': 838,\n",
       "  'triumphant': 839,\n",
       "  'problemo': 840,\n",
       "  'problaly': 841,\n",
       "  'round,': 842,\n",
       "  'wits': 843,\n",
       "  'chistmas': 844,\n",
       "  'gatto': 845,\n",
       "  'paddies': 846,\n",
       "  'know,': 847,\n",
       "  'chicagoians': 848,\n",
       "  'whove': 849,\n",
       "  'costing': 850,\n",
       "  'momentum': 851,\n",
       "  'xiii': 852,\n",
       "  'ess': 853,\n",
       "  'configurations': 854,\n",
       "  'unwell': 855,\n",
       "  'tradititional': 856,\n",
       "  'convenient,': 857,\n",
       "  'aubusson': 858,\n",
       "  'resin': 859,\n",
       "  'coloumn': 860,\n",
       "  'deterioration': 861,\n",
       "  'bah': 862,\n",
       "  'sweep': 863,\n",
       "  'atisaya': 864,\n",
       "  'ginaz': 865,\n",
       "  'exceeded': 866,\n",
       "  'jurassic': 867,\n",
       "  'instanse': 868,\n",
       "  'consummately': 869,\n",
       "  'jeff': 870,\n",
       "  'partner,': 871,\n",
       "  'disgracefulness': 872,\n",
       "  'breathtakig': 873,\n",
       "  'conversations': 874,\n",
       "  'precaution': 875,\n",
       "  'inland': 876,\n",
       "  'position': 877,\n",
       "  'wakes': 878,\n",
       "  'wiser': 879,\n",
       "  'accomadating': 880,\n",
       "  'bottomless': 881,\n",
       "  'prepped': 882,\n",
       "  'malta': 883,\n",
       "  'komische': 884,\n",
       "  'manasquan': 885,\n",
       "  'deere': 886,\n",
       "  'morgue': 887,\n",
       "  'dissing': 888,\n",
       "  'fuc': 889,\n",
       "  'stellar,': 890,\n",
       "  'pickpocketing': 891,\n",
       "  'fulffted': 892,\n",
       "  'balconette': 893,\n",
       "  'redesign': 894,\n",
       "  'rock,': 895,\n",
       "  'bookwe': 896,\n",
       "  'horns': 897,\n",
       "  'calefornia': 898,\n",
       "  'barbecued': 899,\n",
       "  'verona': 900,\n",
       "  'authorities': 901,\n",
       "  'thatcher': 902,\n",
       "  'glossed': 903,\n",
       "  'fevers': 904,\n",
       "  'utans': 905,\n",
       "  'friendlly': 906,\n",
       "  'improves': 907,\n",
       "  'snored': 908,\n",
       "  'allot': 909,\n",
       "  'conrad,': 910,\n",
       "  'chimney': 911,\n",
       "  'dehydration': 912,\n",
       "  'looby': 913,\n",
       "  'nore': 914,\n",
       "  'ember': 915,\n",
       "  'robotic': 916,\n",
       "  'nght': 917,\n",
       "  'confusionthe': 918,\n",
       "  'une': 919,\n",
       "  'soleless': 920,\n",
       "  'wylands': 921,\n",
       "  'zeroom': 922,\n",
       "  'swisshotel': 923,\n",
       "  'antihistamene': 924,\n",
       "  'uprgrading': 925,\n",
       "  'talked,': 926,\n",
       "  'reconcile': 927,\n",
       "  'raped': 928,\n",
       "  'feal': 929,\n",
       "  'bijous': 930,\n",
       "  'orcagna': 931,\n",
       "  'answerd': 932,\n",
       "  'hamburger': 933,\n",
       "  'gigging': 934,\n",
       "  'faciilities': 935,\n",
       "  'checkpoint': 936,\n",
       "  'shat': 937,\n",
       "  'uneatable': 938,\n",
       "  'niceties': 939,\n",
       "  'japanees': 940,\n",
       "  'forgetful': 941,\n",
       "  'toto': 942,\n",
       "  'decorating': 943,\n",
       "  'kerosene': 944,\n",
       "  'marginal': 945,\n",
       "  'earning': 946,\n",
       "  'filtered': 947,\n",
       "  'toilettres': 948,\n",
       "  'raw': 949,\n",
       "  'sauce': 950,\n",
       "  'soars': 951,\n",
       "  'brandy': 952,\n",
       "  'pattys': 953,\n",
       "  'awards': 954,\n",
       "  'puffing': 955,\n",
       "  'oblige,': 956,\n",
       "  'trauma': 957,\n",
       "  'inexistant': 958,\n",
       "  'pulpo': 959,\n",
       "  'uhmmm': 960,\n",
       "  'hibiscus': 961,\n",
       "  'soup': 962,\n",
       "  'expressos': 963,\n",
       "  'inglaterra': 964,\n",
       "  'atmosphere,': 965,\n",
       "  'asheford': 966,\n",
       "  'kicked': 967,\n",
       "  'dinnerangela': 968,\n",
       "  'montemar': 969,\n",
       "  'fires': 970,\n",
       "  'madi': 971,\n",
       "  'khiang': 972,\n",
       "  'gout': 973,\n",
       "  'snootty': 974,\n",
       "  'arraged': 975,\n",
       "  'quiter': 976,\n",
       "  'cinamon': 977,\n",
       "  'detriments': 978,\n",
       "  'opportunity': 979,\n",
       "  'lables': 980,\n",
       "  'spittelmarkt': 981,\n",
       "  'wkends': 982,\n",
       "  'institute': 983,\n",
       "  'sighted': 984,\n",
       "  'ommelettes': 985,\n",
       "  'drinkers': 986,\n",
       "  'attendance': 987,\n",
       "  'katrina': 988,\n",
       "  'souveniere': 989,\n",
       "  'sheesh': 990,\n",
       "  'uncleaned': 991,\n",
       "  'mnac': 992,\n",
       "  'purchases': 993,\n",
       "  'lux': 994,\n",
       "  'sterilize': 995,\n",
       "  'gratefull': 996,\n",
       "  'justify': 997,\n",
       "  'mambas': 998,\n",
       "  'annoys': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ta_size, word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_scifi = set(corpus_scifi.split())\n",
    "vocab_scifi_size = len(vocab_scifi)\n",
    "\n",
    "word_to_ix_scifi = {word: i for i, word in enumerate(vocab_scifi)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the corpus into training and test data \n",
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW:\n",
    "    \n",
    "    def vectorize(self, context_size, corpus):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of context words and the corresponding central words\n",
    "        Args:\n",
    "            contet_size (int): the window length to the left and to the right of the central word\n",
    "            corpus (str): the cleaned corpus as on string, words seperated by space\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = (corpus.split())\n",
    "        for i in range(2, len(corpus_splt) - 2):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW:\n",
    "    \n",
    "    def vectorize(self, context_size, corpus):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of context words and the corresponding central words\n",
    "        Args:\n",
    "            contet_size (int): the window length to the left and to the right of the central word\n",
    "            corpus (str): the cleaned corpus as on string, words seperated by space\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(2, len(corpus_splt) - 2)):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            #context = [corpus_splt[i - 1], corpus_splt[i + 1]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data\n",
    "        \n",
    "        \n",
    "VectCBOW = VectorizerCBOW()\n",
    "\n",
    "cont_targ_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_ta_train)\n",
    "cont_scifi_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOWContextAware:\n",
    "    def __init__(self, context_width):\n",
    "        self.width = context_width\n",
    "    def vectorize(self, context_size, corpus):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of context words and the corresponding central words\n",
    "        Args:\n",
    "            contet_size (int): the window length to the left and to the right of the central word\n",
    "            corpus (str): the cleaned corpus as on string, words seperated by space\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(self.width, len(corpus_splt) - self.width)):\n",
    "            context = corpus_splt[(i - self.width):(i + self.width + 1)]\n",
    "            context.remove(corpus_splt[i])\n",
    "            \"\"\"context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\"\"\"\n",
    "            #context = [corpus_splt[i - 1], corpus_splt[i + 1]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data\n",
    "        \n",
    "        \n",
    "VectCBOW = VectorizerCBOWContextAware(5)\n",
    "\n",
    "cont_targ_train_ca5 = VectCBOW.vectorize(CONTEXT_SIZE, corpus_ta_train)\n",
    "#cont_scifi_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cont_targ_train), cont_targ_train[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, everything works <br>\n",
    "Below, work in progress <br>\n",
    "# Hic sunt dracones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus_ta_train.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "lst = []\n",
    "for x in batch(cont_targ_train, 25):\n",
    "    lst.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #embeds = self.embeddings(inputs).view((inputs.size(0), -1))\n",
    "        \n",
    "        #HERE\n",
    "        #embeds = self.embeddings(inputs).view((1, -1))\n",
    "        \n",
    "        # -1 implies size inferred for that index from the size of the data\n",
    "        #print(np.mean(np.mean(self.linear2.weight.data.numpy())))\n",
    "        #out1 = F.relu()\n",
    "        \n",
    "        #HERE\n",
    "        #out1 = F.relu(self.linear1(embeds)) # output of first layer\n",
    "        \n",
    "        #print(\"Out1: \", str(out1))\n",
    "        \n",
    "        #HERE\n",
    "        #out2 = self.linear2(out1)           # output of second layer\n",
    "        \n",
    "        #print(embeds)\n",
    "        \n",
    "        #HERE\n",
    "        #log_probs = F.log_softmax(out2, dim=1)\n",
    "        #return log_probs\n",
    "        \n",
    "        #return out1\n",
    "        out = self.embeddings(inputs).view(1, -1)\n",
    "        out = out.view(1,-1)\n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in input], dtype=torch.long)\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        res_val = res_val[0][:3]\n",
    "        res_ind = res_ind[0][:3]\n",
    "        #print(res_val)\n",
    "        #print(res_ind)\n",
    "        for arg in zip(res_val,res_ind):\n",
    "            #print(arg)\n",
    "            print([(key,val,arg[0]) for key,val in word_to_ix.items() if val == arg[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(cont_targ_train, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 50\n",
    "print(\"Context size: \", CONTEXT_SIZE)\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_ta_size, EMBED_DIM, CONTEXT_SIZE)\n",
    "#model = NGramLanguageModeler(len(vocab_ta), EMBED_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze embedding layer\n",
    "#model.freeze_layer('embeddings')\n",
    "\n",
    "for epoch in tqdm(range(1)):\n",
    "    total_loss = 0\n",
    "    #------- Embedding layers are trained as well here ----#\n",
    "    #lookup_tensor = torch.tensor([word_to_ix[\"poor\"]], dtype=torch.long)\n",
    "    #hello_embed = model.embeddings(lookup_tensor)\n",
    "    #print(hello_embed)\n",
    "    # -----------------------------------------------------#\n",
    "    for context, target in tqdm(loader):\n",
    "    #for context, target in tqdm(cont_targ_train):\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        #context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "    \n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        #log_probs = model(context_idxs)\n",
    "        log_probs = model(context)\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        \n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "#print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "#Predict the next word given n context words\n",
    "#model.predict(['inside','every','human'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex02_wordembeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
