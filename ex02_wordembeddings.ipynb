{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes\n",
    "In this section, we build classes which are later used to load and clean data. We moved this to the top of the notebook to declutter the following section and to make it easier for you to follow our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-downloading class\n",
    "It has three metods: <br>\n",
    "<ul>\n",
    "    <li><b>__init__(url):</b> Class constructor, takes an URL to GDrive;</li>\n",
    "    <li><b>load_csv():</b> Downloads a .csv from the GDrive link and returns it as pandas dataframe;</li>\n",
    "    <li><b>load_txt():</b> Downloads a .txt file from the GDrive link and returns it as a string.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downloader:\n",
    "    def __init__(self, gdrive_url):\n",
    "        self.path = gdrive_url   \n",
    "    def load_csv(self):\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        url = requests.get(dwn_url).content\n",
    "        csv_raw = StringIO(url.decode('utf-8'))\n",
    "        df_ta = pd.read_csv(csv_raw)\n",
    "        return(df_ta)\n",
    "    def load_txt(self):\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        #print(dwn_url)\n",
    "        url = requests.get(dwn_url).content\n",
    "        scifi = StringIO(url.decode('utf-8')).getvalue()\n",
    "        return(scifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-cleaner class\n",
    "As suggested in McMahan (Natural Language Processing with PyTorch), we have proceeded by lower-casing each sentence, by removing the punctuation and the non alphabetic characters, also the stopwords have been stripped away from the text. <br>\n",
    "The strings get lowercased in order to avoid seeing the words at the beginning of the phrases as different words. <br>\n",
    "Whitespaces are used to tokenize the strings.<br>\n",
    "This class has three methods and no constructor except from the default one: <br>\n",
    "<ul>\n",
    "    <li><b>remove_nonalpha_chars:</b> Takes as input a pandas dataframe and the name of a column, proceeds to remove all non alpha-chars from said column. Returns the cleaned dataframe;</li>\n",
    "    <li><b>lower_casing():</b> Takes a pandas dataframe and the name of the column to operate on, all the uppercase chars become lowercase. Returns the lowercased dataframe;</li>\n",
    "    <li><b>remove_stopwords():</b> Removes all the stopwords from the column of a dataframe, both fields of the function. It returns the cleaned dataframe.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def remove_nonalpha_chars(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \", x))\n",
    "        return df_out\n",
    "    def lower_casing(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: x.lower())\n",
    "        return df_out \n",
    "    def remove_stopwords(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: ' '.join([word for word in (x.split()) if word not in (stop) and len(word) != 1]))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW_Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW: \n",
    "    def vectorize(self, context_size, corpus):        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(2, len(corpus_splt) - 2)):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOWContextWindow:\n",
    "    def __init__(self, context_width):\n",
    "        self.width = context_width\n",
    "    def vectorize(self, context_size, corpus):\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(self.width, len(corpus_splt) - self.width)):\n",
    "            context = corpus_splt[(i - self.width):(i + self.width + 1)]\n",
    "            context.remove(corpus_splt[i])\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        print(str(self.embeddings))\n",
    "        #self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        #self.linear1 = nn.Linear(200, 1)\n",
    "        #print(\"Cont_size * emb_dim: \", context_size * embedding_dim)\n",
    "        #self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(\"FORWARD CALL!\")\n",
    "        #print(\"INPUTS SHAPE: \", inputs.shape)\n",
    "        #embeds = self.embeddings(inputs).view((inputs.size(0), -1))\n",
    "        \n",
    "        #HERE\n",
    "        #embeds = self.embeddings(inputs).view((1, -1))\n",
    "        \n",
    "        #print(\"Embeds type: \", type(embeds), type(self.embeddings(inputs)))\n",
    "        #print(\"Embeds shape / size: \", str(embeds.shape))\n",
    "        #print(\"Embeds: \", str(embeds))\n",
    "        #print(type(embeds))\n",
    "        # -1 implies size inferred for that index from the size of the data\n",
    "        #print(np.mean(np.mean(self.linear2.weight.data.numpy())))\n",
    "        #print(\"First layer output shape: \", str(self.linear1(embeds)))\n",
    "        #out1 = F.relu()\n",
    "        \n",
    "        #HERE\n",
    "        #out1 = F.relu(self.linear1(embeds)) # output of first layer\n",
    "        \n",
    "        #print(\"Out1: \", str(out1))\n",
    "        \n",
    "        #HERE\n",
    "        #out2 = self.linear2(out1)           # output of second layer\n",
    "        \n",
    "        #print(embeds)\n",
    "        \n",
    "        #HERE\n",
    "        #log_probs = F.log_softmax(out2, dim=1)\n",
    "        #return log_probs\n",
    "        \n",
    "        #return out1\n",
    "        out = self.embeddings(inputs).view(1, -1)\n",
    "        out = out.view(1,-1)\n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in input], dtype=torch.long)\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        res_val = res_val[0][:5]\n",
    "        res_ind = res_ind[0][:5]\n",
    "        #print(res_val)\n",
    "        #print(res_ind)\n",
    "        for arg in zip(res_val,res_ind):\n",
    "            #print(arg)\n",
    "            print([(key,val,arg[0]) for key,val in word_to_ix.items() if val == arg[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader class pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "Here the two datasets gets downloaded directly from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### URLS\n",
    "orig_url_ta = 'https://drive.google.com/file/d/1ihP1HZ8YHVGGIEp1RHxXdt3PPIi12xvL/view?usp=sharing'\n",
    "orig_url_scifi = \"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoaders\n",
    "TravAdvDataSetLoader = Downloader(orig_url_ta)\n",
    "ScifiLoader = Downloader(orig_url_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSV and txts\n",
    "df_ta = TravAdvDataSetLoader.load_csv()\n",
    "scifi_txt = ScifiLoader.load_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20491"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_df = scifi_txt.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sci-fi story gets turned into a dataframe to allow a more proper cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_dict = {\"Text\": [scifi_txt]}\n",
    "scifi_df = pd.DataFrame.from_dict(scifi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "\n",
    "1.   Some reveiw include the rating (i.e. 4*). This should be removed\n",
    "2.   The last line has a typo (and probably many other lines too) which add noise. A correction of all errors, however, is not realistic.\n",
    "\n",
    "Now, we look for all characters used in the reviews to get an idea of how we need to preprocess the data. We can see that there are no foreign language characters in the data but a couple of symbols, special characters and emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Data Cleaning\n",
    "For the data preprocessing, we first create a class that helps us to clean the data (following the OOP approach).\n",
    "\n",
    "**Note**: We only perform operations on the complete data set (training + test set) which do not lead to information leakage. Removing certain characters from the test set is a valid operation that also occurs in real world setting. Data is usually preprocessed before predictions are made.\n",
    "After cleaning the data in the step below, we compare the results of the second review to confirm that the cleaning was successful.<br>\n",
    "We realize that most comments contain typos and that some typos like \"did n't\" result in single characters in the corpus. Given that we cannot correct every typo, we accept this noise in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945670c20ff545bf8258783a5ceae3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514bbec76c524480996db69401e33688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d6f7539ed14b549ea7ed826979e765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DtCleaner = DataCleaner()\n",
    "\n",
    "df_ta_cl = DtCleaner.remove_nonalpha_chars(df_ta, 'Review')\n",
    "df_ta_cl = DtCleaner.lower_casing(df_ta_cl, 'Review')\n",
    "df_ta_cl = DtCleaner.remove_stopwords(df_ta_cl, 'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff606dea29c14361a20dd596424b4cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad684808a4924120bf6a5769d7db660c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75744a95462e4e21b3cd8787f0174374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.remove_nonalpha_chars(scifi_df, \"Text\")\n",
    "df_scifi_cl = DtCleaner.lower_casing(df_scifi_cl, \"Text\")\n",
    "df_scifi_cl = DtCleaner.remove_stopwords(df_scifi_cl, \"Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into a training and test set\n",
    "# using set seed to allow replication\n",
    "np.random.seed(123)\n",
    "m = np.random.rand(len(df_ta_cl)) < 0.7\n",
    "n = np.random.rand(len(df_scifi_cl)) < 0.7\n",
    "df_ta_train = df_ta_cl[m]\n",
    "df_ta_test = df_ta_cl[~m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005026597042604"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if split was correct\n",
    "len(df_ta_train) / len(df_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006076"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ta_train = df_ta_train['Review'].str.cat(sep=', ')\n",
    "corpus_ta_test =  df_ta_test['Review'].str.cat(sep=', ')\n",
    "\n",
    "corpus_ta = corpus_ta_train + corpus_ta_test\n",
    "len(corpus_ta.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_scifi = df_scifi_cl[\"Text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
