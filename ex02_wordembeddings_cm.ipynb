{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "idMPWePBWZUB"
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing classes\n",
    "In this section, we build classes which are later used to load and clean data. We moved this to the top of the notebook to declutter the following section and to make it easier for you to follow our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-downloading class\n",
    "It has three metods: <br>\n",
    "<ul>\n",
    "    <li><b>__init__(url):</b> Class constructor, takes an URL to GDrive;</li>\n",
    "    <li><b>load_csv():</b> Downloads a .csv from the GDrive link and returns it as pandas dataframe;</li>\n",
    "    <li><b>load_txt():</b> Downloads a .txt file from the GDrive link and returns it as a string.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to load data\n",
    "# following this stackoverflow post to download directly from Google Drive:\n",
    "# https://stackoverflow.com/a/56611995\n",
    "\n",
    "class Downloader:\n",
    "    def __init__(self, gdrive_url):\n",
    "        self.path = gdrive_url   \n",
    "    def load_csv(self):\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        url = requests.get(dwn_url).content\n",
    "        csv_raw = StringIO(url.decode('utf-8'))\n",
    "        df_ta = pd.read_csv(csv_raw)\n",
    "        return(df_ta)\n",
    "    def load_txt(self):\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        #print(dwn_url)\n",
    "        url = requests.get(dwn_url).content\n",
    "        scifi = StringIO(url.decode('utf-8')).getvalue()\n",
    "        return(scifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-cleaner class\n",
    "As suggested in McMahan (Natural Language Processing with PyTorch), we have proceeded by lower-casing each sentence, by removing the punctuation and the non alphabetic characters, also the stopwords have been stripped away from the text. <br>\n",
    "The strings get lowercased in order to avoid seeing the words at the beginning of the phrases as different words. <br>\n",
    "Whitespaces are used to tokenize the strings.<br>\n",
    "This class has three methods and no constructor except from the default one: <br>\n",
    "<ul>\n",
    "    <li><b>remove_nonalpha_chars:</b> Takes as input a pandas dataframe and the name of a column, proceeds to remove all non alpha-chars from said column. Returns the cleaned dataframe;</li>\n",
    "    <li><b>lower_casing():</b> Takes a pandas dataframe and the name of the column to operate on, all the uppercase chars become lowercase. Returns the lowercased dataframe;</li>\n",
    "    <li><b>remove_stopwords():</b> Removes all the stopwords from the column of a dataframe, both fields of the function. It returns the cleaned dataframe.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tore/venv/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# Stopword removal inspired by this stackoverlow answer\n",
    "# https://stackoverflow.com/a/43407993/7505264\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tqdm.pandas()\n",
    "\n",
    "class DataCleaner:\n",
    "    def remove_nonalpha_chars(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \", x))\n",
    "        return df_out\n",
    "    def lower_casing(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: x.lower())\n",
    "        return df_out \n",
    "    def remove_stopwords(self, df, column):\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: ' '.join([word for word in (x.split()) if word not in (stop) and len(word) != 1]))\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lth8ThtWHuc"
   },
   "source": [
    "# Loading data\n",
    "Here the two datasets gets downloaded directly from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tMgyvheSWKq2"
   },
   "outputs": [],
   "source": [
    "### URLS\n",
    "orig_url_ta = 'https://drive.google.com/file/d/1ihP1HZ8YHVGGIEp1RHxXdt3PPIi12xvL/view?usp=sharing'\n",
    "orig_url_scifi = \"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DataLoaders\n",
    "TravAdvDataSetLoader = DataLoader(orig_url_ta)\n",
    "ScifiLoader = DataLoader(orig_url_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSV and txts\n",
    "df_ta = TravAdvDataSetLoader.load_csv()\n",
    "scifi_txt = ScifiLoader.load_txt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DbD_oHDW0_7"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the datasets have been downloaded correctly\n",
    "This is done by printing in the first case the head of the pandas dataframe, in the second case by printing the first 500 chars of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "DFspOK98W0Nq",
    "outputId": "6f473269-6e50-4c34-8467-def67c1bc926"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20491"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sci-fi story gets turned into a dataframe to allow a more proper cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scifi_dict = {\"Text\": [scifi_txt]}\n",
    "scifi_df = pd.DataFrame.from_dict(scifi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15388019"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scifi_df[\"Text\"].iloc[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJNUSouXZzBI"
   },
   "source": [
    "**Observations**:\n",
    "\n",
    "\n",
    "1.   Some reveiw include the rating (i.e. 4*). This should be removed\n",
    "2.   The last line has a typo (and probably many other lines too) which add noise. A correction of all errors, however, is not realistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "253JGx_BcJN2"
   },
   "source": [
    "Now, we look for all characters used in the reviews to get an idea of how we need to preprocess the data. We can see that there are no foreign language characters in the data but a couple of symbols, special characters and emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JMqXL4FctBZ"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CJVT_mwcwMg"
   },
   "source": [
    "For the data preprocessing, we first create a class that helps us to clean the data (following the OOP approach).\n",
    "\n",
    "**Note**: We only perform operations on the complete data set (training + test set) which do not lead to information leakage. Removing certain characters from the test set is a valid operation that also occurs in real world setting. Data is usually preprocessed before predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data in the step below, we compare the results of the second review to confirm that the cleaning was successful.\n",
    "We realize that most comments contain typos and that some typos like \"did n't\" result in single characters in the corpus. Given that we cannot correct every typo, we accept this noise in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nxy10s1tc9Yx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8810d2050c4db6932273f5cb1fb813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a234776e35447d9a591b3934ded4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd41051e21ce41b49dca3bb916e49d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DtCleaner = DataCleaner()\n",
    "\n",
    "df_ta_cl = DtCleaner.remove_nonalpha_chars(df_ta, 'Review')\n",
    "df_ta_cl = DtCleaner.lower_casing(df_ta_cl, 'Review')\n",
    "df_ta_cl = DtCleaner.remove_stopwords(df_ta_cl, 'Review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee9c4f6f6914038a762a664525f4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.remove_nonalpha_chars(scifi_df, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94b0f0adc7b4c65aa7d04fb74ace1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.lower_casing(df_scifi_cl, \"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7257b245ce244ba9f71322023ddee76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_scifi_cl = DtCleaner.remove_stopwords(df_scifi_cl, \"Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into a training and test set\n",
    "# using set seed to allow replication\n",
    "np.random.seed(123)\n",
    "m = np.random.rand(len(df_ta_cl)) < 0.7\n",
    "n = np.random.rand(len(df_scifi_cl)) < 0.7\n",
    "df_ta_train = df_ta_cl[m]\n",
    "df_ta_test = df_ta_cl[~m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005026597042604"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if split was correct\n",
    "len(df_ta_train) / len(df_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006076"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_ta_train = df_ta_train['Review'].str.cat(sep=', ')\n",
    "corpus_ta_test =  df_ta_test['Review'].str.cat(sep=', ')\n",
    "\n",
    "corpus_ta = corpus_ta_train + corpus_ta_test\n",
    "len(corpus_ta.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_scifi = df_scifi_cl[\"Text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "# FREQUENCIES OF WORDS IN CORPUS\n",
    "sorted_corpus_freqs_ta = sorted(Counter(corpus_ta.split(\" \")).items(), key=lambda x: x[1], reverse=True)\n",
    "plt.plot([(x[1]) for x in sorted_corpus_freqs_ta[:5]]), sorted_corpus_freqs_ta[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_corpus_freqs_scifi = sorted(Counter(corpus_scifi.split(\" \")).items(), key=lambda x: x[1], reverse=True)\n",
    "plt.plot([(x[1]) for x in sorted_corpus_freqs_scifi[:10]]), sorted_corpus_freqs_scifi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(sorted_corpus_freqs_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7001007930355352"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are checking the ration of training to test sample again because we split the dataframe above, not the corpus\n",
    "# in theory, we could have sampled a lot of rows from the DF with long strings and obtained a training set which is more than\n",
    "# 70% of the corpus. highly unlikely given the size of the DF and the random sampling. So just to make sure we got this right.\n",
    "\n",
    "len(corpus_ta_train) / (len(corpus_ta_train) + len(corpus_ta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n voc train: 44016\n",
      "n voc test: 28326\n",
      "n voc combined: 52497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8480"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n voc train: \" + str(len(set(corpus_ta_train.split()))))\n",
    "print(\"n voc test: \" + str(len(set(corpus_ta_test.split()))))\n",
    "\n",
    "print(\"n voc combined: \" + str(len(set((corpus_ta_train + corpus_ta_test).split()))))\n",
    "\n",
    "len(set(corpus_ta_test.split()).difference(set(corpus_ta_train.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary encoding\n",
    "Encoding our vocabulary. We are encoding the full corpus, as suggested in the exercise forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ta = set(corpus_ta.split())\n",
    "vocab_ta_size = len(vocab_ta)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab_ta)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'burnpeople': 0,\n",
       " 'lfferty': 1,\n",
       " 'knit': 2,\n",
       " 'clendon': 3,\n",
       " 'particualr': 4,\n",
       " 'lewissohn': 5,\n",
       " 'everapprcciative': 6,\n",
       " 'hving': 7,\n",
       " 'hquidgolded': 8,\n",
       " 'misdemeanor': 9,\n",
       " 'backus': 10,\n",
       " 'calflength': 11,\n",
       " 'inchingly': 12,\n",
       " 'muriga': 13,\n",
       " 'damaged': 14,\n",
       " 'rotr': 15,\n",
       " 'doubledome': 16,\n",
       " 'lusted': 17,\n",
       " 'suvomese': 18,\n",
       " 'notificaition': 19,\n",
       " 'mcdicouncil': 20,\n",
       " 'hexmanstory': 21,\n",
       " 'colombia': 22,\n",
       " 'coulomb': 23,\n",
       " 'ssa': 24,\n",
       " 'oomph': 25,\n",
       " 'brews': 26,\n",
       " 'prepossessing': 27,\n",
       " 'followthat': 28,\n",
       " 'nsir': 29,\n",
       " 'produced': 30,\n",
       " 'broach': 31,\n",
       " 'therms': 32,\n",
       " 'bhidqsl': 33,\n",
       " 'crossopterygii': 34,\n",
       " 'newscasts': 35,\n",
       " 'felshaw': 36,\n",
       " 'menge': 37,\n",
       " 'subsidences': 38,\n",
       " 'enzymic': 39,\n",
       " 'battery': 40,\n",
       " 'huing': 41,\n",
       " 'cmdl': 42,\n",
       " 'tyires': 43,\n",
       " 'tuckees': 44,\n",
       " 'scjuat': 45,\n",
       " 'bobbin': 46,\n",
       " 'cowpile': 47,\n",
       " 'otear': 48,\n",
       " 'massages': 49,\n",
       " 'warmbludded': 50,\n",
       " 'scary': 51,\n",
       " 'greenrbordered': 52,\n",
       " 'zerog': 53,\n",
       " 'beteach': 54,\n",
       " 'casebook': 55,\n",
       " 'eater': 56,\n",
       " 'awwwwkk': 57,\n",
       " 'todbots': 58,\n",
       " 'imsisspw': 59,\n",
       " 'subception': 60,\n",
       " 'eu': 61,\n",
       " 'loiig': 62,\n",
       " 'pravda': 63,\n",
       " 'screwball': 64,\n",
       " 'leavin': 65,\n",
       " 'displease': 66,\n",
       " 'happened': 67,\n",
       " 'cajoled': 68,\n",
       " 'flensing': 69,\n",
       " 'regularization': 70,\n",
       " 'pummelled': 71,\n",
       " 'bloodrcoloced': 72,\n",
       " 'aboutwashington': 73,\n",
       " 'stilloperating': 74,\n",
       " 'metes': 75,\n",
       " 'unidentifiable': 76,\n",
       " 'otherworld': 77,\n",
       " 'undegstand': 78,\n",
       " 'ganap': 79,\n",
       " 'encumbrance': 80,\n",
       " 'pupae': 81,\n",
       " 'andeans': 82,\n",
       " 'counterplan': 83,\n",
       " 'kerrel': 84,\n",
       " 'dismrbance': 85,\n",
       " 'biochemist': 86,\n",
       " 'woise': 87,\n",
       " 'dunol': 88,\n",
       " 'undercoating': 89,\n",
       " 'vulgo': 90,\n",
       " 'relock': 91,\n",
       " 'paradises': 92,\n",
       " 'mentary': 93,\n",
       " 'bandylegged': 94,\n",
       " 'brakes': 95,\n",
       " 'davidson': 96,\n",
       " 'philiac': 97,\n",
       " 'scientese': 98,\n",
       " 'eadi': 99,\n",
       " 'blued': 100,\n",
       " 'sirs': 101,\n",
       " 'mtnical': 102,\n",
       " 'antiearthmen': 103,\n",
       " 'hoping': 104,\n",
       " 'leik': 105,\n",
       " 'godl': 106,\n",
       " 'headover': 107,\n",
       " 'cozies': 108,\n",
       " 'assumption': 109,\n",
       " 'scalded': 110,\n",
       " 'mareh': 111,\n",
       " 'deepcarpeted': 112,\n",
       " 'gegenbauer': 113,\n",
       " 'headeventually': 114,\n",
       " 'reagent': 115,\n",
       " 'thigbs': 116,\n",
       " 'pletely': 117,\n",
       " 'nrosinase': 118,\n",
       " 'hiirt': 119,\n",
       " 'scape': 120,\n",
       " 'bacteriophysicist': 121,\n",
       " 'stingers': 122,\n",
       " 'etre': 123,\n",
       " 'deed': 124,\n",
       " 'roadways': 125,\n",
       " 'halides': 126,\n",
       " 'tucanite': 127,\n",
       " 'stints': 128,\n",
       " 'coup': 129,\n",
       " 'carves': 130,\n",
       " 'weapoik': 131,\n",
       " 'incarceratoriums': 132,\n",
       " 'autocar': 133,\n",
       " 'irrespknsible': 134,\n",
       " 'eachj': 135,\n",
       " 'supremes': 136,\n",
       " 'jimson': 137,\n",
       " 'contestants': 138,\n",
       " 'usk': 139,\n",
       " 'butdetermined': 140,\n",
       " 'yledonous': 141,\n",
       " 'sulphury': 142,\n",
       " 'wodans': 143,\n",
       " 'lustiest': 144,\n",
       " 'rockin': 145,\n",
       " 'aimi': 146,\n",
       " 'lfbok': 147,\n",
       " 'agonizedly': 148,\n",
       " 'examinedv': 149,\n",
       " 'rqected': 150,\n",
       " 'dcdkled': 151,\n",
       " 'abstractionist': 152,\n",
       " 'jri': 153,\n",
       " 'hellcat': 154,\n",
       " 'iiti': 155,\n",
       " 'serpentining': 156,\n",
       " 'humanmanned': 157,\n",
       " 'paraphysicist': 158,\n",
       " 'courteously': 159,\n",
       " 'stranlund': 160,\n",
       " 'syndicalist': 161,\n",
       " 'rff': 162,\n",
       " 'zoos': 163,\n",
       " 'screwdrivers': 164,\n",
       " 'wastingl': 165,\n",
       " 'nike': 166,\n",
       " 'stopnow': 167,\n",
       " 'ecky': 168,\n",
       " 'demetrios': 169,\n",
       " 'guth': 170,\n",
       " 'mary': 171,\n",
       " 'omega': 172,\n",
       " 'honkytonk': 173,\n",
       " 'icily': 174,\n",
       " 'fbirole': 175,\n",
       " 'enlaced': 176,\n",
       " 'ravielli': 177,\n",
       " 'sufferer': 178,\n",
       " 'shoulders': 179,\n",
       " 'multisonic': 180,\n",
       " 'tiirew': 181,\n",
       " 'accoununcy': 182,\n",
       " 'cdisus': 183,\n",
       " 'sull': 184,\n",
       " 'diesem': 185,\n",
       " 'limqa': 186,\n",
       " 'peddling': 187,\n",
       " 'earthshine': 188,\n",
       " 'gibraltar': 189,\n",
       " 'ciae': 190,\n",
       " 'magnap': 191,\n",
       " 'overfondness': 192,\n",
       " 'rockahominy': 193,\n",
       " 'chesapeake': 194,\n",
       " 'ignium': 195,\n",
       " 'magistero': 196,\n",
       " 'interdis': 197,\n",
       " 'asmike': 198,\n",
       " 'deinfect': 199,\n",
       " 'liigh': 200,\n",
       " 'fcood': 201,\n",
       " 'nphis': 202,\n",
       " 'reappearing': 203,\n",
       " 'infantryman': 204,\n",
       " 'prompt': 205,\n",
       " 'oldlooking': 206,\n",
       " 'sgt': 207,\n",
       " 'ores': 208,\n",
       " 'individualists': 209,\n",
       " 'capjbain': 210,\n",
       " 'menthly': 211,\n",
       " 'phes': 212,\n",
       " 'experi': 213,\n",
       " 'whammy': 214,\n",
       " 'rise': 215,\n",
       " 'khachaturian': 216,\n",
       " 'driveway': 217,\n",
       " 'backstop': 218,\n",
       " 'barminess': 219,\n",
       " 'signficantly': 220,\n",
       " 'rogan': 221,\n",
       " 'concentwate': 222,\n",
       " 'somatics': 223,\n",
       " 'sopie': 224,\n",
       " 'schwarz': 225,\n",
       " 'processl': 226,\n",
       " 'exhil': 227,\n",
       " 'reknirhsdaeh': 228,\n",
       " 'adag': 229,\n",
       " 'staylit': 230,\n",
       " 'fleshwould': 231,\n",
       " 'blarn': 232,\n",
       " 'register': 233,\n",
       " 'cackled': 234,\n",
       " 'handmingled': 235,\n",
       " 'rumand': 236,\n",
       " 'faneweedi': 237,\n",
       " 'mandl': 238,\n",
       " 'gemmate': 239,\n",
       " 'kiddiebutt': 240,\n",
       " 'tralaser': 241,\n",
       " 'fairlv': 242,\n",
       " 'wvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvmvvvvvvvmvvvvvvvvmvvvvvvvuvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvj': 243,\n",
       " 'scalnia': 244,\n",
       " 'lapaz': 245,\n",
       " 'ieh': 246,\n",
       " 'exceediny': 247,\n",
       " 'fantasist': 248,\n",
       " 'superspace': 249,\n",
       " 'ultravidet': 250,\n",
       " 'mosses': 251,\n",
       " 'nucleons': 252,\n",
       " 'naintongue': 253,\n",
       " 'mess': 254,\n",
       " 'wonderingiy': 255,\n",
       " 'mossy': 256,\n",
       " 'willoughby': 257,\n",
       " 'cct': 258,\n",
       " 'resti': 259,\n",
       " 'skymaster': 260,\n",
       " 'vthat': 261,\n",
       " 'opaquely': 262,\n",
       " 'episcopal': 263,\n",
       " 'livened': 264,\n",
       " 'menticom': 265,\n",
       " 'jungly': 266,\n",
       " 'saurian': 267,\n",
       " 'museek': 268,\n",
       " 'arbor': 269,\n",
       " 'tepped': 270,\n",
       " 'clangings': 271,\n",
       " 'dra': 272,\n",
       " 'altimeter': 273,\n",
       " 'priestessjudge': 274,\n",
       " 'aloafg': 275,\n",
       " 'lorluwus': 276,\n",
       " 'ambled': 277,\n",
       " 'deg': 278,\n",
       " 'homed': 279,\n",
       " 'ped': 280,\n",
       " 'bcause': 281,\n",
       " 'abacus': 282,\n",
       " 'pansfcat': 283,\n",
       " 'vhatever': 284,\n",
       " 'dieuy': 285,\n",
       " 'lelley': 286,\n",
       " 'juror': 287,\n",
       " 'alcorian': 288,\n",
       " 'liva': 289,\n",
       " 'blaspheming': 290,\n",
       " 'sequitur': 291,\n",
       " 'fiavor': 292,\n",
       " 'simeon': 293,\n",
       " 'darjeeling': 294,\n",
       " 'selecans': 295,\n",
       " 'clicy': 296,\n",
       " 'safetyvalve': 297,\n",
       " 'falklands': 298,\n",
       " 'prefix': 299,\n",
       " 'flocks': 300,\n",
       " 'jnedicinal': 301,\n",
       " 'cita': 302,\n",
       " 'marparli': 303,\n",
       " 'numbwit': 304,\n",
       " 'serrengian': 305,\n",
       " 'malingerer': 306,\n",
       " 'slitch': 307,\n",
       " 'purpkse': 308,\n",
       " 'unwitnessed': 309,\n",
       " 'landbound': 310,\n",
       " 'starpafhers': 311,\n",
       " 'greenstreet': 312,\n",
       " 'lending': 313,\n",
       " 'hemispheric': 314,\n",
       " 'se': 315,\n",
       " 'chalo': 316,\n",
       " 'ligjhits': 317,\n",
       " 'opalescent': 318,\n",
       " 'reardoni': 319,\n",
       " 'keelis': 320,\n",
       " 'worldmen': 321,\n",
       " 'dependables': 322,\n",
       " 'turningtoward': 323,\n",
       " 'dosn': 324,\n",
       " 'maving': 325,\n",
       " 'wenches': 326,\n",
       " 'sitari': 327,\n",
       " 'loorie': 328,\n",
       " 'ourohorous': 329,\n",
       " 'balustraded': 330,\n",
       " 'taumtowal': 331,\n",
       " 'vty': 332,\n",
       " 'tkat': 333,\n",
       " 'cient': 334,\n",
       " 'igr': 335,\n",
       " 'jie': 336,\n",
       " 'dare': 337,\n",
       " 'distempering': 338,\n",
       " 'thurm': 339,\n",
       " 'albsorption': 340,\n",
       " 'coresponsor': 341,\n",
       " 'krotalu': 342,\n",
       " 'legort': 343,\n",
       " 'toti': 344,\n",
       " 'hoopskirt': 345,\n",
       " 'nowp': 346,\n",
       " 'chavezian': 347,\n",
       " 'eleesa': 348,\n",
       " 'dimiber': 349,\n",
       " 'flonnie': 350,\n",
       " 'ffexibility': 351,\n",
       " 'imprimatur': 352,\n",
       " 'slipstreaming': 353,\n",
       " 'pharasang': 354,\n",
       " 'haptens': 355,\n",
       " 'crass': 356,\n",
       " 'htslth': 357,\n",
       " 'medicouncilor': 358,\n",
       " 'lifecycle': 359,\n",
       " 'ophthalmology': 360,\n",
       " 'cosmotheoros': 361,\n",
       " 'poc': 362,\n",
       " 'itlight': 363,\n",
       " 'jag': 364,\n",
       " 'paragon': 365,\n",
       " 'cornsies': 366,\n",
       " 'dnmkenly': 367,\n",
       " 'domineering': 368,\n",
       " 'addid': 369,\n",
       " 'shyst': 370,\n",
       " 'tobyt': 371,\n",
       " 'techboth': 372,\n",
       " 'porter': 373,\n",
       " 'catchbasin': 374,\n",
       " 'ibecame': 375,\n",
       " 'deepfreeze': 376,\n",
       " 'torte': 377,\n",
       " 'heliport': 378,\n",
       " 'iews': 379,\n",
       " 'laitghler': 380,\n",
       " 'mlirley': 381,\n",
       " 'abovea': 382,\n",
       " 'charta': 383,\n",
       " 'deadline': 384,\n",
       " 'disbict': 385,\n",
       " 'ithipped': 386,\n",
       " 'compliance': 387,\n",
       " 'gecko': 388,\n",
       " 'xnt': 389,\n",
       " 'shoidder': 390,\n",
       " 'dorrance': 391,\n",
       " 'overendways': 392,\n",
       " 'trattoria': 393,\n",
       " 'debilitate': 394,\n",
       " 'ibear': 395,\n",
       " 'overemphasis': 396,\n",
       " 'tales': 397,\n",
       " 'flimsies': 398,\n",
       " 'toground': 399,\n",
       " 'wjern': 400,\n",
       " 'ravens': 401,\n",
       " 'akhund': 402,\n",
       " 'hydroxethyl': 403,\n",
       " 'thyrano': 404,\n",
       " 'rosecrystal': 405,\n",
       " 'subcribe': 406,\n",
       " 'ushering': 407,\n",
       " 'clickpops': 408,\n",
       " 'braked': 409,\n",
       " 'tipper': 410,\n",
       " 'baycon': 411,\n",
       " 'lawbooks': 412,\n",
       " 'bref': 413,\n",
       " 'penology': 414,\n",
       " 'ohjectise': 415,\n",
       " 'sorn': 416,\n",
       " 'childlike': 417,\n",
       " 'estidollars': 418,\n",
       " 'clicksclickedwill': 419,\n",
       " 'shield': 420,\n",
       " 'herby': 421,\n",
       " 'enlarging': 422,\n",
       " 'worder': 423,\n",
       " 'amperage': 424,\n",
       " 'nucleomitrophobia': 425,\n",
       " 'closerto': 426,\n",
       " 'altalr': 427,\n",
       " 'rauscher': 428,\n",
       " 'dismounted': 429,\n",
       " 'regu': 430,\n",
       " 'otruding': 431,\n",
       " 'liglft': 432,\n",
       " 'beoring': 433,\n",
       " 'herbivora': 434,\n",
       " 'relocated': 435,\n",
       " 'nfortunately': 436,\n",
       " 'lmery': 437,\n",
       " 'apricots': 438,\n",
       " 'coffins': 439,\n",
       " 'remortal': 440,\n",
       " 'theq': 441,\n",
       " 'unkown': 442,\n",
       " 'throatclearing': 443,\n",
       " 'mossman': 444,\n",
       " 'snoll': 445,\n",
       " 'wdlman': 446,\n",
       " 'minerology': 447,\n",
       " 'noe': 448,\n",
       " 'reon': 449,\n",
       " 'bogger': 450,\n",
       " 'tactfut': 451,\n",
       " 'fielded': 452,\n",
       " 'gulfport': 453,\n",
       " 'veram': 454,\n",
       " 'unnerved': 455,\n",
       " 'stagger': 456,\n",
       " 'om': 457,\n",
       " 'ocs': 458,\n",
       " 'incandessuddenly': 459,\n",
       " 'viscount': 460,\n",
       " 'yang': 461,\n",
       " 'cambodian': 462,\n",
       " 'aggressive': 463,\n",
       " 'twict': 464,\n",
       " 'otnei': 465,\n",
       " 'dims': 466,\n",
       " 'experif': 467,\n",
       " 'earthwave': 468,\n",
       " 'clothet': 469,\n",
       " 'silverste': 470,\n",
       " 'cliarming': 471,\n",
       " 'hateything': 472,\n",
       " 'toiiimself': 473,\n",
       " 'unintended': 474,\n",
       " 'bradman': 475,\n",
       " 'bocediga': 476,\n",
       " 'pigeons': 477,\n",
       " 'lattorkni': 478,\n",
       " 'proclivity': 479,\n",
       " 'thodora': 480,\n",
       " 'pubescence': 481,\n",
       " 'breck': 482,\n",
       " 'plantation': 483,\n",
       " 'reassures': 484,\n",
       " 'turied': 485,\n",
       " 'ejchanging': 486,\n",
       " 'unfortun': 487,\n",
       " 'dumps': 488,\n",
       " 'purscmaker': 489,\n",
       " 'pery': 490,\n",
       " 'seemefr': 491,\n",
       " 'nddlotilno': 492,\n",
       " 'cosmographia': 493,\n",
       " 'mabel': 494,\n",
       " 'rluimni': 495,\n",
       " 'impassable': 496,\n",
       " 'purveyors': 497,\n",
       " 'entrapment': 498,\n",
       " 'shodowy': 499,\n",
       " 'awakening': 500,\n",
       " 'focussing': 501,\n",
       " 'quantities': 502,\n",
       " 'tranquil': 503,\n",
       " 'chrysanthemums': 504,\n",
       " 'holborn': 505,\n",
       " 'cinamo': 506,\n",
       " 'wowee': 507,\n",
       " 'ungrandladylike': 508,\n",
       " 'spacebound': 509,\n",
       " 'sproul': 510,\n",
       " 'acroaa': 511,\n",
       " 'netiher': 512,\n",
       " 'quarthat': 513,\n",
       " 'entropic': 514,\n",
       " 'girlthings': 515,\n",
       " 'devotedly': 516,\n",
       " 'desmids': 517,\n",
       " 'haunters': 518,\n",
       " 'unsealed': 519,\n",
       " 'intuitive': 520,\n",
       " 'embarassed': 521,\n",
       " 'intermingle': 522,\n",
       " 'rity': 523,\n",
       " 'jaksin': 524,\n",
       " 'havely': 525,\n",
       " 'scramblers': 526,\n",
       " 'geneticdesirability': 527,\n",
       " 'freaks': 528,\n",
       " 'brooksand': 529,\n",
       " 'ggy': 530,\n",
       " 'jennet': 531,\n",
       " 'thermo': 532,\n",
       " 'matical': 533,\n",
       " 'praecursor': 534,\n",
       " 'dizzied': 535,\n",
       " 'coverage': 536,\n",
       " 'antic': 537,\n",
       " 'sigillarias': 538,\n",
       " 'revisionist': 539,\n",
       " 'alaxlesl': 540,\n",
       " 'tically': 541,\n",
       " 'nutsi': 542,\n",
       " 'fwiend': 543,\n",
       " 'dumbness': 544,\n",
       " 'misreading': 545,\n",
       " 'attack': 546,\n",
       " 'kdatlyno': 547,\n",
       " 'hurler': 548,\n",
       " 'norrn': 549,\n",
       " 'inhumanness': 550,\n",
       " 'ajpstem': 551,\n",
       " 'mittee': 552,\n",
       " 'whilejhe': 553,\n",
       " 'shapelessness': 554,\n",
       " 'betterheeled': 555,\n",
       " 'broadaxe': 556,\n",
       " 'reftlly': 557,\n",
       " 'simulalion': 558,\n",
       " 'skrit': 559,\n",
       " 'qjupiter': 560,\n",
       " 'eithor': 561,\n",
       " 'allentown': 562,\n",
       " 'zwick': 563,\n",
       " 'iears': 564,\n",
       " 'metnoa': 565,\n",
       " 'gleamif': 566,\n",
       " 'screarned': 567,\n",
       " 'youinterstellferederhydrowithgenlawmassfulexecufrom': 568,\n",
       " 'bomb': 569,\n",
       " 'exhaustion': 570,\n",
       " 'spiked': 571,\n",
       " 'adjuring': 572,\n",
       " 'variety': 573,\n",
       " 'unrational': 574,\n",
       " 'fountainhead': 575,\n",
       " 'silverberg': 576,\n",
       " 'prel': 577,\n",
       " 'bergenfleld': 578,\n",
       " 'improbable': 579,\n",
       " 'nightdwellers': 580,\n",
       " 'fnstftutfon': 581,\n",
       " 'aready': 582,\n",
       " 'clemens': 583,\n",
       " 'poising': 584,\n",
       " 'bruth': 585,\n",
       " 'issve': 586,\n",
       " 'tiyitarigold': 587,\n",
       " 'fde': 588,\n",
       " 'pitch': 589,\n",
       " 'cloner': 590,\n",
       " 'cbjined': 591,\n",
       " 'nooty': 592,\n",
       " 'flames': 593,\n",
       " 'pigheaded': 594,\n",
       " 'opimsile': 595,\n",
       " 'scokh': 596,\n",
       " 'mythoxian': 597,\n",
       " 'galla': 598,\n",
       " 'bullyboys': 599,\n",
       " 'lowpowered': 600,\n",
       " 'eld': 601,\n",
       " 'clau': 602,\n",
       " 'inattendon': 603,\n",
       " 'peereing': 604,\n",
       " 'turbot': 605,\n",
       " 'dcstruct': 606,\n",
       " 'shavings': 607,\n",
       " 'peculiar': 608,\n",
       " 'bottlehots': 609,\n",
       " 'tbp': 610,\n",
       " 'airound': 611,\n",
       " 'deftness': 612,\n",
       " 'eshave': 613,\n",
       " 'copflat': 614,\n",
       " 'afom': 615,\n",
       " 'beguiling': 616,\n",
       " 'husted': 617,\n",
       " 'humdn': 618,\n",
       " 'roud': 619,\n",
       " 'harama': 620,\n",
       " 'notches': 621,\n",
       " 'sarong': 622,\n",
       " 'feghoot': 623,\n",
       " 'bowdlerization': 624,\n",
       " 'qitting': 625,\n",
       " 'aurorae': 626,\n",
       " 'chronically': 627,\n",
       " 'bluechips': 628,\n",
       " 'torbothan': 629,\n",
       " 'ihsh': 630,\n",
       " 'glook': 631,\n",
       " 'wheelhouse': 632,\n",
       " 'yeaaaa': 633,\n",
       " 'andsilent': 634,\n",
       " 'spacefleets': 635,\n",
       " 'rafngtraiioo': 636,\n",
       " 'statos': 637,\n",
       " 'littlechanged': 638,\n",
       " 'oommodore': 639,\n",
       " 'klein': 640,\n",
       " 'terranormal': 641,\n",
       " 'paycheck': 642,\n",
       " 'bellingrath': 643,\n",
       " 'schulz': 644,\n",
       " 'diners': 645,\n",
       " 'reconditioner': 646,\n",
       " 'coquette': 647,\n",
       " 'aiula': 648,\n",
       " 'wolfhounds': 649,\n",
       " 'prared': 650,\n",
       " 'leptic': 651,\n",
       " 'drritde': 652,\n",
       " 'sandler': 653,\n",
       " 'aptitudes': 654,\n",
       " 'base': 655,\n",
       " 'hakstrott': 656,\n",
       " 'inadvertantly': 657,\n",
       " 'unethical': 658,\n",
       " 'merchants': 659,\n",
       " 'sioux': 660,\n",
       " 'superddphin': 661,\n",
       " 'projfect': 662,\n",
       " 'oims': 663,\n",
       " 'teacups': 664,\n",
       " 'advantageous': 665,\n",
       " 'jine': 666,\n",
       " 'sugarless': 667,\n",
       " 'albother': 668,\n",
       " 'rocsates': 669,\n",
       " 'topranking': 670,\n",
       " 'jonesy': 671,\n",
       " 'lionel': 672,\n",
       " 'surround': 673,\n",
       " 'hima': 674,\n",
       " 'mortimer': 675,\n",
       " 'ierfect': 676,\n",
       " 'poor': 677,\n",
       " 'teaclier': 678,\n",
       " 'legencj': 679,\n",
       " 'durrell': 680,\n",
       " 'runners': 681,\n",
       " 'videotape': 682,\n",
       " 'tarper': 683,\n",
       " 'borriy': 684,\n",
       " 'chavez': 685,\n",
       " 'forgethce': 686,\n",
       " 'imnecessarily': 687,\n",
       " 'opensecret': 688,\n",
       " 'eardi': 689,\n",
       " 'equal': 690,\n",
       " 'kemridge': 691,\n",
       " 'mcdonnell': 692,\n",
       " 'enjdyed': 693,\n",
       " 'urhousen': 694,\n",
       " 'snappishly': 695,\n",
       " 'looka': 696,\n",
       " 'wheedling': 697,\n",
       " 'othecs': 698,\n",
       " 'ravin': 699,\n",
       " 'noint': 700,\n",
       " 'icers': 701,\n",
       " 'eartlnncn': 702,\n",
       " 'portland': 703,\n",
       " 'torchlighters': 704,\n",
       " 'jonasori': 705,\n",
       " 'hoppeni': 706,\n",
       " 'drecklich': 707,\n",
       " 'deeduction': 708,\n",
       " 'receivers': 709,\n",
       " 'sailed': 710,\n",
       " 'pockmark': 711,\n",
       " 'dalictha': 712,\n",
       " 'hatchways': 713,\n",
       " 'geologie': 714,\n",
       " 'administer': 715,\n",
       " 'uku': 716,\n",
       " 'lifeblood': 717,\n",
       " 'protosunlight': 718,\n",
       " 'instryiments': 719,\n",
       " 'narrowest': 720,\n",
       " 'milla': 721,\n",
       " 'unamiable': 722,\n",
       " 'annoyance': 723,\n",
       " 'cower': 724,\n",
       " 'troublet': 725,\n",
       " 'indetectable': 726,\n",
       " 'wingnut': 727,\n",
       " 'wingbeats': 728,\n",
       " 'integrally': 729,\n",
       " 'knifepwint': 730,\n",
       " 'mished': 731,\n",
       " 'nonhypnos': 732,\n",
       " 'brx': 733,\n",
       " 'whangin': 734,\n",
       " 'beeton': 735,\n",
       " 'dustier': 736,\n",
       " 'woolf': 737,\n",
       " 'darkcheckered': 738,\n",
       " 'jilanets': 739,\n",
       " 'florentines': 740,\n",
       " 'cadamosto': 741,\n",
       " 'nipoff': 742,\n",
       " 'smallbrain': 743,\n",
       " 'placable': 744,\n",
       " 'hospitali': 745,\n",
       " 'sponsons': 746,\n",
       " 'photovision': 747,\n",
       " 'froon': 748,\n",
       " 'painfuihy': 749,\n",
       " 'church': 750,\n",
       " 'suicided': 751,\n",
       " 'hominoid': 752,\n",
       " 'halitosic': 753,\n",
       " 'mercator': 754,\n",
       " 'macbeth': 755,\n",
       " 'sloppery': 756,\n",
       " 'chaainiry': 757,\n",
       " 'lifeguards': 758,\n",
       " 'ongin': 759,\n",
       " 'educating': 760,\n",
       " 'limpskinned': 761,\n",
       " 'beebe': 762,\n",
       " 'addicts': 763,\n",
       " 'locket': 764,\n",
       " 'brushless': 765,\n",
       " 'unawkward': 766,\n",
       " 'lanthanotus': 767,\n",
       " 'tumuli': 768,\n",
       " 'anadt': 769,\n",
       " 'leniniststalinist': 770,\n",
       " 'helipet': 771,\n",
       " 'wagons': 772,\n",
       " 'ctm': 773,\n",
       " 'pushy': 774,\n",
       " 'pluribus': 775,\n",
       " 'mccartys': 776,\n",
       " 'abopt': 777,\n",
       " 'carbonized': 778,\n",
       " 'aintest': 779,\n",
       " 'centurions': 780,\n",
       " 'clockwis': 781,\n",
       " 'gertips': 782,\n",
       " 'hrodenu': 783,\n",
       " 'arlor': 784,\n",
       " 'rankiing': 785,\n",
       " 'oftowners': 786,\n",
       " 'pinmiles': 787,\n",
       " 'presumalaborator': 788,\n",
       " 'veraiklaii': 789,\n",
       " 'balanites': 790,\n",
       " 'ebullism': 791,\n",
       " 'unforgivably': 792,\n",
       " 'thythtem': 793,\n",
       " 'exiedient': 794,\n",
       " 'literalness': 795,\n",
       " 'alreatly': 796,\n",
       " 'sacredncss': 797,\n",
       " 'shtdl': 798,\n",
       " 'patriotic': 799,\n",
       " 'invincibly': 800,\n",
       " 'camelopardist': 801,\n",
       " 'arbund': 802,\n",
       " 'sunbathed': 803,\n",
       " 'breached': 804,\n",
       " 'santesson': 805,\n",
       " 'venie': 806,\n",
       " 'xcoth': 807,\n",
       " 'unfulfilled': 808,\n",
       " 'slimg': 809,\n",
       " 'soooooo': 810,\n",
       " 'vibra': 811,\n",
       " 'consequential': 812,\n",
       " 'ceavenltnco': 813,\n",
       " 'devoutly': 814,\n",
       " 'imagined': 815,\n",
       " 'padlocked': 816,\n",
       " 'fakash': 817,\n",
       " 'photocheck': 818,\n",
       " 'sfunned': 819,\n",
       " 'prepackaged': 820,\n",
       " 'higherparaffin': 821,\n",
       " 'highwalled': 822,\n",
       " 'siths': 823,\n",
       " 'filibustered': 824,\n",
       " 'rm': 825,\n",
       " 'booklist': 826,\n",
       " 'eartliside': 827,\n",
       " 'holm': 828,\n",
       " 'frow': 829,\n",
       " 'deteriorates': 830,\n",
       " 'sunsailing': 831,\n",
       " 'wtas': 832,\n",
       " 'sightings': 833,\n",
       " 'abbreviating': 834,\n",
       " 'gangway': 835,\n",
       " 'que': 836,\n",
       " 'unseasonably': 837,\n",
       " 'darnedest': 838,\n",
       " 'triurriph': 839,\n",
       " 'nucleii': 840,\n",
       " 'sweetcakes': 841,\n",
       " 'supervisors': 842,\n",
       " 'kerned': 843,\n",
       " 'mutally': 844,\n",
       " 'longsuffering': 845,\n",
       " 'okayama': 846,\n",
       " 'alreadyexisted': 847,\n",
       " 'astreet': 848,\n",
       " 'metatunderneath': 849,\n",
       " 'whitish': 850,\n",
       " 'ethanol': 851,\n",
       " 'mirrorcolor': 852,\n",
       " 'chargedto': 853,\n",
       " 'clotted': 854,\n",
       " 'gerous': 855,\n",
       " 'jazztone': 856,\n",
       " 'tarras': 857,\n",
       " 'quetzal': 858,\n",
       " 'smearing': 859,\n",
       " 'stris': 860,\n",
       " 'nwide': 861,\n",
       " 'lizard': 862,\n",
       " 'propellants': 863,\n",
       " 'anthologizations': 864,\n",
       " 'appeared': 865,\n",
       " 'spectively': 866,\n",
       " 'exultation': 867,\n",
       " 'arnett': 868,\n",
       " 'otest': 869,\n",
       " 'pjb': 870,\n",
       " 'hasheesh': 871,\n",
       " 'ohnttered': 872,\n",
       " 'liixury': 873,\n",
       " 'diverged': 874,\n",
       " 'asconel': 875,\n",
       " 'spokenacted': 876,\n",
       " 'letter': 877,\n",
       " 'octave': 878,\n",
       " 'undersides': 879,\n",
       " 'aliuut': 880,\n",
       " 'ggure': 881,\n",
       " 'saidt': 882,\n",
       " 'sleeplike': 883,\n",
       " 'asleep': 884,\n",
       " 'asinine': 885,\n",
       " 'maisie': 886,\n",
       " 'albanian': 887,\n",
       " 'pathogen': 888,\n",
       " 'fhiat': 889,\n",
       " 'flustrated': 890,\n",
       " 'work': 891,\n",
       " 'moranites': 892,\n",
       " 'baffles': 893,\n",
       " 'mementos': 894,\n",
       " 'hpt': 895,\n",
       " 'wideawake': 896,\n",
       " 'hamiel': 897,\n",
       " 'psyche': 898,\n",
       " 'nightmarishly': 899,\n",
       " 'whqe': 900,\n",
       " 'foldy': 901,\n",
       " 'birkenia': 902,\n",
       " 'wander': 903,\n",
       " 'kneelers': 904,\n",
       " 'wudly': 905,\n",
       " 'circumstances': 906,\n",
       " 'holmesing': 907,\n",
       " 'minon': 908,\n",
       " 'eliminates': 909,\n",
       " 'harems': 910,\n",
       " 'ptoases': 911,\n",
       " 'hoarseiy': 912,\n",
       " 'cresentlal': 913,\n",
       " 'letup': 914,\n",
       " 'defendable': 915,\n",
       " 'plots': 916,\n",
       " 'abuilding': 917,\n",
       " 'sete': 918,\n",
       " 'scratchmarks': 919,\n",
       " 'sceptorlike': 920,\n",
       " 'cacophony': 921,\n",
       " 'gns': 922,\n",
       " 'pmqwe': 923,\n",
       " 'begg': 924,\n",
       " 'toanbdartrbltoii': 925,\n",
       " 'maximizing': 926,\n",
       " 'sellin': 927,\n",
       " 'koztn': 928,\n",
       " 'nero': 929,\n",
       " 'unemployable': 930,\n",
       " 'kts': 931,\n",
       " 'caim': 932,\n",
       " 'zmi': 933,\n",
       " 'lugard': 934,\n",
       " 'innkeepers': 935,\n",
       " 'unstriding': 936,\n",
       " 'soil': 937,\n",
       " 'grasshoppery': 938,\n",
       " 'broadshouldered': 939,\n",
       " 'robotronics': 940,\n",
       " 'jackspurt': 941,\n",
       " 'oanvas': 942,\n",
       " 'importanceeasily': 943,\n",
       " 'fourshot': 944,\n",
       " 'trophies': 945,\n",
       " 'sktian': 946,\n",
       " 'saphrophytism': 947,\n",
       " 'cadences': 948,\n",
       " 'cellsurgery': 949,\n",
       " 'imique': 950,\n",
       " 'jetcopters': 951,\n",
       " 'grimmando': 952,\n",
       " 'sleg': 953,\n",
       " 'duker': 954,\n",
       " 'absolutdy': 955,\n",
       " 'woii': 956,\n",
       " 'fardistant': 957,\n",
       " 'treat': 958,\n",
       " 'superinduce': 959,\n",
       " 'twopronged': 960,\n",
       " 'clamor': 961,\n",
       " 'enlivening': 962,\n",
       " 'centrist': 963,\n",
       " 'browsers': 964,\n",
       " 'ziggledy': 965,\n",
       " 'triplet': 966,\n",
       " 'redeems': 967,\n",
       " 'itp': 968,\n",
       " 'excusable': 969,\n",
       " 'barnoll': 970,\n",
       " 'blasphemous': 971,\n",
       " 'mitman': 972,\n",
       " 'slowiy': 973,\n",
       " 'obstructionist': 974,\n",
       " 'construetion': 975,\n",
       " 'handshaped': 976,\n",
       " 'brotherin': 977,\n",
       " 'lithians': 978,\n",
       " 'oveaeer': 979,\n",
       " 'specked': 980,\n",
       " 'inoperation': 981,\n",
       " 'davidi': 982,\n",
       " 'cuiller': 983,\n",
       " 'unstarched': 984,\n",
       " 'goaf': 985,\n",
       " 'vermiformoidy': 986,\n",
       " 'actimi': 987,\n",
       " 'iate': 988,\n",
       " 'poppa': 989,\n",
       " 'ahriman': 990,\n",
       " 'prie': 991,\n",
       " 'corselet': 992,\n",
       " 'elimination': 993,\n",
       " 'adorned': 994,\n",
       " 'hbw': 995,\n",
       " 'titbe': 996,\n",
       " 'structuring': 997,\n",
       " 'threedays': 998,\n",
       " 'crackiny': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_scifi = set(corpus_scifi.split())\n",
    "vocab_scifi_size = len(vocab_scifi)\n",
    "\n",
    "word_to_ix_scifi = {word: i for i, word in enumerate(vocab_scifi)}\n",
    "word_to_ix_scifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the corpus into training and test data \n",
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW: \n",
    "    def vectorize(self, context_size, corpus):        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(2, len(corpus_splt) - 2)):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b88062486234f5fa47d1d8ae9def4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1404241.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2964ed46b154a9a8565750f226e09cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7699160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VectCBOW = VectorizerCBOW()\n",
    "\n",
    "cont_targ_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_ta_train)\n",
    "cont_scifi_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOWContextWindow:\n",
    "    def __init__(self, context_width):\n",
    "        self.width = context_width\n",
    "    def vectorize(self, context_size, corpus):\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in tqdm(range(self.width, len(corpus_splt) - self.width)):\n",
    "            context = corpus_splt[(i - self.width):(i + self.width + 1)]\n",
    "            context.remove(corpus_splt[i])\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7e52cf2c5d4de797c719f625a660d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1404235.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda0ac8c6029494fb497d325888fb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7699154.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VectCBOW_contwind = VectorizerCBOWContextWindow(5)\n",
    "\n",
    "cont_targ_train_ca5 = VectCBOW_contwind.vectorize(CONTEXT_SIZE, corpus_ta_train)\n",
    "cont_scifi_train_ca5 = VectCBOW_contwind.vectorize(CONTEXT_SIZE, corpus_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1404241,\n",
       " [(['nice', 'hotel', 'parking', 'got'], 'expensive'),\n",
       "  (['hotel', 'expensive', 'got', 'good'], 'parking'),\n",
       "  (['expensive', 'parking', 'good', 'deal'], 'got')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_targ_train), cont_targ_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(cont_targ_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cont_targ_train is too big to be used\n",
    "Taking a randomly shuffled subset to make everything work. <br>\n",
    "It will affect embedding quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_tripad = cont_targ_train[:int(len(cont_targ_train)*0.10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140424,\n",
       " [(['chic', 'comfortable', 'appointment', 'room'], 'lighting'),\n",
       "  (['blast', 'recommend', 'fun', 'added'], 'join'),\n",
       "  (['parasailing', 'stuff', 'waiting', 'hours'], 'leave')])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_tripad), subset_tripad[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, everything works <br>\n",
    "Below, work in progress <br>\n",
    "# Hic sunt dracones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        print(str(self.embeddings))\n",
    "        #self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        #self.linear1 = nn.Linear(200, 1)\n",
    "        #print(\"Cont_size * emb_dim: \", context_size * embedding_dim)\n",
    "        #self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(\"FORWARD CALL!\")\n",
    "        #print(\"INPUTS SHAPE: \", inputs.shape)\n",
    "        #embeds = self.embeddings(inputs).view((inputs.size(0), -1))\n",
    "        \n",
    "        #HERE\n",
    "        #embeds = self.embeddings(inputs).view((1, -1))\n",
    "        \n",
    "        #print(\"Embeds type: \", type(embeds), type(self.embeddings(inputs)))\n",
    "        #print(\"Embeds shape / size: \", str(embeds.shape))\n",
    "        #print(\"Embeds: \", str(embeds))\n",
    "        #print(type(embeds))\n",
    "        # -1 implies size inferred for that index from the size of the data\n",
    "        #print(np.mean(np.mean(self.linear2.weight.data.numpy())))\n",
    "        #print(\"First layer output shape: \", str(self.linear1(embeds)))\n",
    "        #out1 = F.relu()\n",
    "        \n",
    "        #HERE\n",
    "        #out1 = F.relu(self.linear1(embeds)) # output of first layer\n",
    "        \n",
    "        #print(\"Out1: \", str(out1))\n",
    "        \n",
    "        #HERE\n",
    "        #out2 = self.linear2(out1)           # output of second layer\n",
    "        \n",
    "        #print(embeds)\n",
    "        \n",
    "        #HERE\n",
    "        #log_probs = F.log_softmax(out2, dim=1)\n",
    "        #return log_probs\n",
    "        \n",
    "        #return out1\n",
    "        out = self.embeddings(inputs).view(1, -1)\n",
    "        out = out.view(1,-1)\n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in input], dtype=torch.long)\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        res_val = res_val[0][:5]\n",
    "        res_ind = res_ind[0][:5]\n",
    "        #print(res_val)\n",
    "        #print(res_ind)\n",
    "        for arg in zip(res_val,res_ind):\n",
    "            #print(arg)\n",
    "            print([(key,val,arg[0]) for key,val in word_to_ix.items() if val == arg[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context size:  2\n",
      "Embedding(52497, 50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6513efed3870414ea79053d864c2dfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e8ce11726041138be2abd734c35cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBED_DIM = 50\n",
    "print(\"Context size: \", CONTEXT_SIZE)\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_ta_size, EMBED_DIM, CONTEXT_SIZE)\n",
    "#model = NGramLanguageModeler(len(vocab_ta), EMBED_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Freeze embedding layer\n",
    "#model.freeze_layer('embeddings')\n",
    "\n",
    "for epoch in tqdm(range(25)):\n",
    "    print(epoch)\n",
    "    total_loss = 0\n",
    "    #------- Embedding layers are trained as well here ----#\n",
    "    #lookup_tensor = torch.tensor([word_to_ix[\"poor\"]], dtype=torch.long)\n",
    "    #hello_embed = model.embeddings(lookup_tensor)\n",
    "    #print(hello_embed)\n",
    "    # -----------------------------------------------------#\n",
    "    i = 0\n",
    "    for context, target in tqdm(subset_tripad[:15000]):\n",
    "        #print(\"Progress: {0}/{1}\".format(i, len(cont_targ_train)))\n",
    "        #i += 1\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        #print(\"Context id: \", context_idxs)\n",
    "        #print(\"Context id shape: \", context_idxs.shape)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        #print(model)\n",
    "        log_probs = model(context_idxs)\n",
    "        #print(log_probs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        #print(loss)\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "#print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([\"inside\", \"every\", \"human\", \"rainbow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex02_wordembeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
