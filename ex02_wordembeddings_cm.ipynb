{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "idMPWePBWZUB"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "from io import StringIO\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if pytorch can use the gpu\n",
    "\n",
    "# print(torch.cuda.current_device())\n",
    "# print(torch.cuda.device(0))\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "# print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build classes which are later used to load and clean data. We moved this to the top of the notebook to declutter the following section and to make it easier for you to follow our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to load data\n",
    "# following this stackoverflow post to download directly from Google Drive:\n",
    "# https://stackoverflow.com/a/56611995\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, gdrive_url):\n",
    "        \"\"\"\n",
    "        Class constructor\n",
    "        Args:\n",
    "            gdrive_url (str): URL to share Google Drive docs\n",
    "        \"\"\"\n",
    "        self.path = gdrive_url   \n",
    "    def load_csv(self):\n",
    "        \"\"\"\n",
    "        Returns DataFrame containing data from csv\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        url = requests.get(dwn_url).content\n",
    "        csv_raw = StringIO(url.decode('utf-8'))\n",
    "        df_ta = pd.read_csv(csv_raw)\n",
    "        return(df_ta)\n",
    "    \n",
    "    def load_txt(self):\n",
    "        \"\"\"\n",
    "        Returns String containing data from txt\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        file_id = self.path.split('/')[-2]\n",
    "        dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "        #print(dwn_url)\n",
    "        url = requests.get(dwn_url).content\n",
    "        scifi = StringIO(url.decode('utf-8')).getvalue()\n",
    "        return(scifi)\n",
    "\n",
    "#txt = DataLoader(\"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\").load_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tore/venv/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# class to clean data\n",
    "# when cleaning the text data, we proceed as suggested in McMahan (Natural Language Processing with PyTorch)\n",
    "# by converting each sentence to lowercase and removing the punctuation completely. other non-alpha characters are also removed \n",
    "# white space between words is used to split the strings based on it to retrieve a list of tokens\n",
    "\n",
    "# stopword removal inspired by this stackoverlow answer\n",
    "# https://stackoverflow.com/a/43407993/7505264\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "tqdm.pandas()\n",
    "\n",
    "class DataCleaner:\n",
    "    # this method removes non alpha characters like '#', ';', ',','.','/' etc.\n",
    "    def remove_nonalpha_chars(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Removes non-alpha characters\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        \n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \", x))\n",
    "        return df_out\n",
    "    \n",
    "    def lower_casing(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Turns strings to lower case\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        \n",
    "        df_out = df.copy(deep=True)\n",
    "        # we turn all words to lower case to avoid that words that are at the beginning of a sentence are seen as different words\n",
    "        # since we are dealing with English language which is not capitalizing nouns \n",
    "        # However, proper nouns are capitalized in English languange\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: x.lower())\n",
    "        return df_out\n",
    "    \n",
    "    def remove_stopwords(self, df, column):\n",
    "        \"\"\"\n",
    "        Returns a processed DataFrame, does not change original\n",
    "        Removes stopwords from text in English language\n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the corpus\n",
    "            column (str): Name of the DF column with text data (corpus)\n",
    "        \"\"\"\n",
    "        df_out = df.copy(deep=True)\n",
    "        df_out[column] = df_out[column].progress_apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "        return df_out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing class\n",
    "# df_ta['Review'].apply(lambda x: re.sub(r\"\\s*[^A-Za-z]+\\s*\", \" \",x))\n",
    "\n",
    "# rvws = {'Review': ['TEST_.']}\n",
    "\n",
    "# df_test = pd.DataFrame(rvws, columns = ['Review'])\n",
    "\n",
    "# df_test['Review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer class\n",
    "class CBOWVectorizer:\n",
    "    \"\"\" Class that coordinates Vocabularies by vectorizing \"\"\"\n",
    "    \n",
    "    def vectorize(self, df, column, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (DataFrame): dataframe containing the text samples\n",
    "            column (str): name of the column that contains text samples\n",
    "            window_size (int): length of the word window \n",
    "                (2 means 2 leading and 2 trailing words, hence, 4 context words)\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lth8ThtWHuc"
   },
   "source": [
    "\n",
    "# Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tMgyvheSWKq2"
   },
   "outputs": [],
   "source": [
    "# url for tripadvisor_hotel_reviews.csv\n",
    "# following this stackoverflow post to download directly from Google Drive:\n",
    "# https://stackoverflow.com/a/56611995\n",
    "\n",
    "### URLS\n",
    "orig_url_ta = 'https://drive.google.com/file/d/1ihP1HZ8YHVGGIEp1RHxXdt3PPIi12xvL/view?usp=sharing'\n",
    "orig_url_scifi = \"https://drive.google.com/file/d/10ehW4jZND3QA29v9aNboYUett5-swuNe/view?usp=sharing\"\n",
    "\n",
    "### DataLoaders\n",
    "TravAdvDataSetLoader = DataLoader(orig_url_ta)\n",
    "ScifiLoader = DataLoader(orig_url_scifi)\n",
    "\n",
    "### CSV and txts\n",
    "df_ta = TravAdvDataSetLoader.load_csv()\n",
    "scifi_txt = ScifiLoader.load_txt()\n",
    "\n",
    "# file_id = orig_url_ta.split('/')[-2]\n",
    "# dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
    "# url = requests.get(dwn_url).content\n",
    "# csv_raw = StringIO(url.decode('utf-8'))\n",
    "# df_ta = pd.read_csv(csv_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DbD_oHDW0_7"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qMC01F5ZcLy"
   },
   "source": [
    "Just checking if the download of the csv was successful by looking at the first and last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "DFspOK98W0Nq",
    "outputId": "6f473269-6e50-4c34-8467-def67c1bc926"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  nice hotel expensive parking got good deal sta...       4\n",
       "1  ok nothing special charge diamond member hilto...       2\n",
       "2  nice rooms not 4* experience hotel monaco seat...       3\n",
       "3  unique, great stay, wonderful time hotel monac...       5\n",
       "4  great stay great stay, went seahawk game aweso...       5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJNUSouXZzBI"
   },
   "source": [
    "**Observations**:\n",
    "\n",
    "\n",
    "1.   Some reveiw include the rating (i.e. 4*). This should be removed\n",
    "2.   The last line has a typo (and probably many other lines too) which add noise. A correction of all errors, however, is not realistic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "253JGx_BcJN2"
   },
   "source": [
    "Now, we look for all characters used in the reviews to get an idea of how we need to preprocess the data. We can see that there are no foreign language characters in the data but a couple of symbols, special characters and emojis.\n",
    "\n",
    "(this takes a minute to run, so skip this cell if you are in a hurry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wErFVjIdZjIh",
    "outputId": "103dcb39-dc3e-4458-aa59-b262079dcb91"
   },
   "outputs": [],
   "source": [
    "# the function concatenates all strings and then builds a set of unique characters\n",
    "# np.array(df_ta.Review.sum()).reshape(-1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXVz9Xk2knvG"
   },
   "source": [
    "**Observations**:\n",
    "\n",
    "We see that there are certain non-alpha signs, for example â‚¬ or $, which could add value to the word embedding as they actually stand for Euro or Dollar and make a lot of sense in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JMqXL4FctBZ"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CJVT_mwcwMg"
   },
   "source": [
    "For the data preprocessing, we first create a class that helps us to clean the data (following the OOP approach).\n",
    "\n",
    "**Note**: We only perform operations on the complete data set (training + test set) which do not lead to information leakage. Removing certain characters from the test set is a valid operation that also occurs in real world setting. Data is usually preprocessed before predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data in the step below, we compare the results of the second review to confirm that the cleaning was successful.\n",
    "We realize that most comments contain typos and that some typos like \"did n't\" result in single characters in the corpus. Given that we cannot correct every typo, we accept this noise in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nxy10s1tc9Yx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa05a99d8e944f5aae250ac9e91d127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5124c9c8c6ec4aefafee2ca67d909611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3865d88f88094c0da6581bfc4a4d2a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20491.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok nothing special charge diamond member hilton decided chain shot th anniversary seattle start booked suite paid extra website description suite bedroom bathroom standard hotel room took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast got kidding embassy suits sitting room bathroom bedroom unlike kimpton calls suite day stay offer correct false advertising send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty reply solution send email trip guest survey follow email mail guess tell concerned guest staff ranged indifferent helpful asked desk good breakfast spots neighborhood hood told hotels gee best breakfast spots seattle block away convenient hotel know exist arrived late night pm inside run bellman busy chating cell phone help bags prior arrival emailed hotel inform th anniversary half really picky wanted make sure good got nice email saying like deliver bottle champagne chocolate covered strawberries room arrival celebrate told needed foam pillows arrival champagne strawberries foam pillows great room view alley high rise building good better housekeeping staff cleaner room property impressed left morning shopping room got short trips hours beds comfortable good ac heat control x inch screen bring green shine directly eyes night light sensitive tape controls start hotel clean business hotel super high rates better chain hotels seattle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DtCleaner = DataCleaner()\n",
    "\n",
    "df_ta_cl = DtCleaner.remove_nonalpha_chars(df_ta, 'Review')\n",
    "df_ta_cl = DtCleaner.lower_casing(df_ta_cl, 'Review')\n",
    "df_ta_cl = DtCleaner.remove_stopwords(df_ta_cl, 'Review')\n",
    "\n",
    "df_ta_cl.iloc[1]['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into a training and test set\n",
    "# using set seed to allow replication\n",
    "np.random.seed(123)\n",
    "m = np.random.rand(len(df_ta_cl)) < 0.7\n",
    "\n",
    "df_ta_train = df_ta_cl[m]\n",
    "df_ta_test = df_ta_cl[~m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005026597042604"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if split was correct\n",
    "len(df_ta_train) / len(df_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ta_train = df_ta_train['Review'].str.cat(sep=', ')\n",
    "corpus_ta_test =  df_ta_test['Review'].str.cat(sep=', ')\n",
    "\n",
    "corpus_ta = corpus_ta_train + corpus_ta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7001140124630911"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are checking the ration of training to test sample again because we split the dataframe above, not the corpus\n",
    "# in theory, we could have sampled a lot of rows from the DF with long strings and obtained a training set which is more than\n",
    "# 70% of the corpus. highly unlikely given the size of the DF and the random sampling. So just to make sure we got this right.\n",
    "\n",
    "len(corpus_ta_train) / (len(corpus_ta_train) + len(corpus_ta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n voc train: 44035\n",
      "n voc test: 28346\n",
      "n voc combined: 52512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8476"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"n voc train: \" + str(len(set(corpus_ta_train.split()))))\n",
    "print(\"n voc test: \" + str(len(set(corpus_ta_test.split()))))\n",
    "\n",
    "print(\"n voc combined: \" + str(len(set((corpus_ta_train + corpus_ta_test).split()))))\n",
    "\n",
    "len(set(corpus_ta_test.split()).difference(set(corpus_ta_train.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"vectorize_layer = TextVectorization(\\n    standardize=custom_standardization,\\n    max_tokens=vocab_size,\\n    output_mode='int',\\n    output_sequence_length=sequence_length)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "\"\"\"vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\"\"\"\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "#text_ds = train_ds.map(lambda x, y: x)\n",
    "#vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding our vocabulary. We are encoding the full corpus, as suggested in the exercise forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ta = set(corpus_ta.split())\n",
    "vocab_ta_size = len(vocab_ta)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab_ta)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the corpus into training and test data \n",
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW:\n",
    "    \n",
    "    def vectorize(self, context_size, corpus):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of context words and the corresponding central words\n",
    "        Args:\n",
    "            contet_size (int): the window length to the left and to the right of the central word\n",
    "            corpus (str): the cleaned corpus as on string, words seperated by space\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in range(2, len(corpus_splt) - 2):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizerCBOW:\n",
    "    \n",
    "    def vectorize(self, context_size, corpus):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            list of context words and the corresponding central words\n",
    "        Args:\n",
    "            contet_size (int): the window length to the left and to the right of the central word\n",
    "            corpus (str): the cleaned corpus as on string, words seperated by space\n",
    "        \"\"\"\n",
    "        \n",
    "        # first, extract the context words and the corresponding central words\n",
    "        data = []\n",
    "        corpus_splt = corpus.split()\n",
    "        for i in range(2, len(corpus_splt) - 2):\n",
    "            context = [corpus_splt[i - 2], corpus_splt[i - 1],\n",
    "                       corpus_splt[i + 1], corpus_splt[i + 2]]\n",
    "            target = corpus_splt[i]\n",
    "            data.append((context, target))\n",
    "        \n",
    "        return data\n",
    "        \n",
    "        \n",
    "VectCBOW = VectorizerCBOW()\n",
    "\n",
    "cont_targ_train = VectCBOW.vectorize(CONTEXT_SIZE, corpus_ta_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, everything works\n",
    "Below, work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))  \n",
    "        # -1 implies size inferred for that index from the size of the data\n",
    "        #print(np.mean(np.mean(self.linear2.weight.data.numpy())))\n",
    "        out1 = F.relu(self.linear1(embeds)) # output of first layer\n",
    "        out2 = self.linear2(out1)           # output of second layer\n",
    "        #print(embeds)\n",
    "        log_probs = F.log_softmax(out2, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in input], dtype=torch.long)\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending=True)\n",
    "        res_val = res_val[0][:3]\n",
    "        res_ind = res_ind[0][:3]\n",
    "        #print(res_val)\n",
    "        #print(res_ind)\n",
    "        for arg in zip(res_val,res_ind):\n",
    "            #print(arg)\n",
    "            print([(key,val,arg[0]) for key,val in word_to_ix.items() if val == arg[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 200], m2: [100 x 128] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a222576ecf29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Step 3. Run the forward pass, getting log probabilities over next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(log_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b68d2b652f98>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(np.mean(np.mean(self.linear2.weight.data.numpy())))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output of first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# output of second layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(embeds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 200], m2: [100 x 128] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 50\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_ta_size, EMBED_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Freeze embedding layer\n",
    "#model.freeze_layer('embeddings')\n",
    "\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    #------- Embedding layers are trained as well here ----#\n",
    "    #lookup_tensor = torch.tensor([word_to_ix[\"poor\"]], dtype=torch.long)\n",
    "    #hello_embed = model.embeddings(lookup_tensor)\n",
    "    #print(hello_embed)\n",
    "    # -----------------------------------------------------#\n",
    "    for context, target in cont_targ_train:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        #print(\"Context id\",context_idxs)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        #print(log_probs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        #print(loss)\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    losses.append(total_loss)\n",
    "#print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "#Print the model layer parameters\n",
    "#model.print_layer_parameters()\n",
    "\n",
    "#Predict the next word given n context words\n",
    "model.predict(['of','all','human'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex02_wordembeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
